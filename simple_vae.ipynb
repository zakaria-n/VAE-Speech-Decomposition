{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhMtBKfvVtBa"
      },
      "source": [
        "# Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.8.0-cp39-cp39-macosx_10_14_x86_64.whl (217.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 217.5 MB 15 kB/s \n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp39-cp39-macosx_10_14_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 2.9 MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.4.0\n",
            "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
            "Collecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 582 kB/s \n",
            "\u001b[?25hCollecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 2.6 MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.1\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.44.0-cp39-cp39-macosx_10_10_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 10.7 MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.13.3-cp39-cp39-macosx_10_9_x86_64.whl (33 kB)\n",
            "Collecting flatbuffers>=1.12\n",
            "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting h5py>=2.9.0\n",
            "  Downloading h5py-3.6.0-cp39-cp39-macosx_10_9_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 6.3 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.20\n",
            "  Downloading numpy-1.22.3-cp39-cp39-macosx_10_14_x86_64.whl (17.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.6 MB 56 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /Users/zak/miniconda3/lib/python3.9/site-packages (from tensorflow) (52.0.0.post20210125)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting libclang>=9.0.1\n",
            "  Downloading libclang-13.0.0-py2.py3-none-macosx_10_9_x86_64.whl (13.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0 MB 86 kB/s \n",
            "\u001b[?25hCollecting protobuf>=3.9.2\n",
            "  Downloading protobuf-3.19.4-cp39-cp39-macosx_10_9_x86_64.whl (961 kB)\n",
            "\u001b[K     |████████████████████████████████| 961 kB 955 kB/s \n",
            "\u001b[?25hCollecting astunparse>=1.6.0\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting typing-extensions>=3.6.6\n",
            "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/zak/miniconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting gast>=0.2.1\n",
            "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/zak/miniconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/zak/miniconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.25.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Using cached tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
            "\u001b[K     |████████████████████████████████| 289 kB 1.0 MB/s \n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Downloading importlib_metadata-4.11.2-py3-none-any.whl (17 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Using cached zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Users/zak/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/zak/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/zak/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/zak/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (4.0.0)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 1.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=e54875419dee7f07b9d7ce451591e69b8281812bf35c4460489fb3355aef39bf\n",
            "  Stored in directory: /Users/zak/Library/Caches/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
            "Successfully built termcolor\n",
            "Installing collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
            "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 h5py-3.6.0 importlib-metadata-4.11.2 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 numpy-1.22.3 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.1.1 werkzeug-2.0.3 wrapt-1.13.3 zipp-3.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.8.0-cp38-cp38-macosx_10_14_x86_64.whl (217.4 MB)\n",
            "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TNDZuTKn2wdI"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.python'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#!pip install tensorflow-gpu==2.3.1\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda, Cropping2D, ZeroPadding2D\n",
            "File \u001b[0;32m~/miniconda3/envs/speechtech/lib/python3.8/site-packages/tensorflow/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "#!pip install tensorflow-gpu==2.3.1\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda, Cropping2D, ZeroPadding2D\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjAK7LB3VyVC"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN3x7QtB4DUf"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/My Drive/vae-speech'\n",
        "\n",
        "data = np.load(dir + '/male.npy', allow_pickle=True)\n",
        "\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWpgUQrkV05O"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gn1sVgEP7G4"
      },
      "outputs": [],
      "source": [
        "def update_dimension():\n",
        "  new_data = []\n",
        "\n",
        "  for idx, wav in enumerate(data):\n",
        "    d = np.delete(data[idx], -1, axis=0)\n",
        "    z = np.zeros((len(d), 1))\n",
        "    d = np.append(d, z, axis = 1)\n",
        "    new_data.append(d)\n",
        "\n",
        "  return np.array(new_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Byx_Wt9VTRO"
      },
      "outputs": [],
      "source": [
        "nd = update_dimension()\n",
        "n = len(nd)\n",
        "\n",
        "#reshape the data\n",
        "nd = np.reshape(nd, (len(nd), 512, 64, 1))\n",
        "\n",
        "\n",
        "x_train = nd[:int(0.6*n)]\n",
        "x_test = nd[int(-0.4*n):]\n",
        "\n",
        "print(\"new dimension: \" + str(nd.shape))\n",
        "print(\"training size: \" + str(len(x_train)))\n",
        "print(\"testing size: \" + str(len(x_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Auvg8DDsV44M"
      },
      "source": [
        "# Model generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ohoppyrKAJk"
      },
      "outputs": [],
      "source": [
        "shape = 1\n",
        "\n",
        "learning_rate = 0.001 #@param {type:\"raw\"}\n",
        "num_epochs_to_train = 30 #@param {type:\"integer\"}\n",
        "batch_size = 32 #@param {type:\"integer\"}\n",
        "vector_dimension = 64 #@param {type:\"integer\"}\n",
        "\n",
        "sampling_rate = 16000 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VAE:\n",
        "  def __init__(self,\n",
        "               input_shape, #shape of the input data\n",
        "               conv_filters, #convolutional network filters\n",
        "               conv_kernels, #convNet kernel size\n",
        "               conv_strides, #convNet strides\n",
        "               latent_space_dim,\n",
        "               foi,\n",
        "               total_MSE = 0,\n",
        "               ):\n",
        "    self.input_shape = input_shape # (513, 63)\n",
        "    self.conv_filters = conv_filters # is a list for each layer, i.e. [2, 4, 8]\n",
        "    self.conv_kernels = conv_kernels # list of kernels per layer, [1, 2, 3]\n",
        "    self.conv_strides = conv_strides # stride for each filter [1, 2, 2], note: 2 means you are downsampling the data in half\n",
        "    self.latent_space_dim = latent_space_dim # how many neurons on bottleneck\n",
        "    self.reconstruction_loss_weight = 1000000\n",
        "    self.total_MSE = total_MSE\n",
        "\n",
        "    self.encoder = None\n",
        "    self.decoder = None\n",
        "    self.model = None\n",
        "    self.hist = None\n",
        "\n",
        "    self._num_conv_layers = len(conv_filters)\n",
        "    self._shape_before_bottleneck = None\n",
        "    self._model_input = None\n",
        "\n",
        "    self._build()\n",
        "\n",
        "  def summary(self):\n",
        "    self.encoder.summary()\n",
        "    print(\"\\n\")\n",
        "    self.decoder.summary()\n",
        "    print(\"\\n\")\n",
        "    self.model.summary()\n",
        "\n",
        "  def _build(self):\n",
        "    self._build_encoder()\n",
        "    self._build_decoder()\n",
        "    self._build_autoencoder()\n",
        "\n",
        "  def compile(self, learning_rate=0.0001):\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    self.model.compile(optimizer=optimizer, loss=self._calculate_combined_loss,\n",
        "                      metrics=[self._calculate_reconstruction_loss, self._calculate_kl_loss])\n",
        "  \n",
        "  def train(self, x_train, batch_size, num_epochs):\n",
        "    self.hist= self.model.fit(x_train,\n",
        "                              x_train,\n",
        "                              batch_size=batch_size,\n",
        "                              epochs=num_epochs,\n",
        "                              shuffle=True)\n",
        "\n",
        "  def _calculate_combined_loss(self, y_target, y_predicted):\n",
        "    reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
        "    kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
        "    foi_loss = self._calculate_foi_loss(y_target, y_predicted)\n",
        "    combined_loss = self.reconstruction_loss_weight * reconstruction_loss + kl_loss + foi_loss + self.total_MSE\n",
        "    return combined_loss\n",
        "  \n",
        "  def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
        "    error = y_target - y_predicted\n",
        "    reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "    return reconstruction_loss\n",
        "\n",
        "  def _calculate_kl_loss(self, y_target, y_predicted):\n",
        "    kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) -\n",
        "                          K.exp(self.log_variance), axis =1)\n",
        "    return kl_loss\n",
        "\n",
        "  def _calculate_foi_loss (self, y_target, y_predicted):   # foi: feature of interest (f0/formants)\n",
        "    if self.foi == \"f0\":\n",
        "      foi_target = f0_array(y_target, \"male\")\n",
        "      foi_predicted = f0_array(y_predicted, \"male\")\n",
        "    else:\n",
        "      foi_target = formants_array(y_target, \"male\")\n",
        "      foi_predicted = formants_array(y_predicted, \"male\")\n",
        "    error = foi_target - foi_predicted\n",
        "    foi_loss = K.mean(K.square(error), axis=[1, 2, 3])  # TODO: axis?\n",
        "    return foi_loss \n",
        "  \n",
        "  def save(self, save_folder):\n",
        "    self._create_folder_if_it_doesnt_exist(save_folder)\n",
        "    self._save_parameters(save_folder)\n",
        "    self._save_weights(save_folder)\n",
        "\n",
        "  def _create_folder_if_it_doesnt_exist(self, folder):\n",
        "      if not os.path.exists(folder):\n",
        "          os.makedirs(folder)\n",
        "\n",
        "  def _save_parameters(self, save_folder):\n",
        "      parameters = [\n",
        "          self.input_shape,\n",
        "          self.conv_filters,\n",
        "          self.conv_kernels,\n",
        "          self.conv_strides,\n",
        "          self.latent_space_dim\n",
        "      ]\n",
        "      save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "      with open(save_path, \"wb\") as f:\n",
        "          pickle.dump(parameters, f)\n",
        "\n",
        "  def _save_weights(self, save_folder):\n",
        "      save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "      self.model.save_weights(save_path)\n",
        "\n",
        "  #----------------FULL MODEL-----------------#\n",
        "  def _build_autoencoder(self):\n",
        "    model_input = self._model_input\n",
        "    model_output = self.decoder(self.encoder(model_input))\n",
        "    self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
        "    return model_output\n",
        "\n",
        "  #----------------DECODER-----------------#\n",
        "  def _build_decoder(self):\n",
        "    decoder_input = self._add_decoder_input()\n",
        "    dense_layer = self._add_dense_layer(decoder_input)\n",
        "    reshape_layer = self._add_reshape_layer(dense_layer)\n",
        "    conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
        "    decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
        "    self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "\n",
        "  def _add_decoder_input(self):\n",
        "    return Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
        "\n",
        "  def _add_dense_layer(self, decoder_input):\n",
        "    num_neurons = np.prod(self._shape_before_bottleneck) # [ 1, 2, 4] -> 8\n",
        "    dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
        "    return dense_layer\n",
        "\n",
        "  def _add_reshape_layer(self, dense_layer):\n",
        "    return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
        "\n",
        "  def _add_conv_transpose_layers(self, x):\n",
        "    for layer_index in reversed(range(1, self._num_conv_layers)):\n",
        "      x = self._add_conv_transpose_layer(layer_index, x)\n",
        "    return x\n",
        "\n",
        "  def _add_conv_transpose_layer(self, layer_index, x):\n",
        "    layer_num = self._num_conv_layers - layer_index\n",
        "    conv_transpose_layer = Conv2DTranspose(\n",
        "        filters=self.conv_filters[layer_index],\n",
        "        kernel_size = self.conv_kernels[layer_index],\n",
        "        strides = self.conv_strides[layer_index],\n",
        "        padding = \"same\",\n",
        "        name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
        "    )\n",
        "    x = conv_transpose_layer(x)\n",
        "    x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
        "    x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
        "    return x\n",
        "\n",
        "  def _add_decoder_output(self, x):\n",
        "    conv_transpose_layer = Conv2DTranspose(\n",
        "        filters = 1,\n",
        "        kernel_size = self.conv_kernels[0],\n",
        "        strides = self.conv_strides[0],\n",
        "        padding = \"same\",\n",
        "        name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
        "    )\n",
        "    x = conv_transpose_layer(x)\n",
        "    output_layer = Activation(\"sigmoid\", name=\"sigmoid_output_layer\")(x)\n",
        "    return output_layer\n",
        "\n",
        "  #----------------ENCODER-----------------#\n",
        "  def _build_encoder(self):\n",
        "    encoder_input = self._add_encoder_input()\n",
        "    # x = Cropping2D(cropping=((1, 0), (0,0)))(encoder_input)\n",
        "    # x = ZeroPadding2D(padding=((0, 0), (1,0)))(x)\n",
        "    conv_layers = self._add_conv_layers(encoder_input)\n",
        "    bottleneck =  self._add_bottleneck(conv_layers)\n",
        "    self._model_input = encoder_input\n",
        "    self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
        "\n",
        "  def _add_encoder_input(self):\n",
        "    return Input(shape=self.input_shape, name=\"encoder_input\")\n",
        "\n",
        "  def _add_conv_layers(self, encoder_input):\n",
        "    \"\"\"Creates all convolutional blocks in encoder\"\"\"\n",
        "    x = encoder_input\n",
        "    for layer_index in range(self._num_conv_layers):\n",
        "      x = self._add_conv_layer(layer_index, x)\n",
        "    return x\n",
        "  \n",
        "  def _add_conv_layer(self, layer_index, x):\n",
        "    \"\"\"\n",
        "    Adds a convolutional block to a graph of layers, consisting\n",
        "    of Conv 2d + ReLu activation + batch normalization.\n",
        "    \"\"\"\n",
        "    layer_number = layer_index + 1\n",
        "    conv_layer = Conv2D(\n",
        "        filters= self.conv_filters[layer_index],\n",
        "        kernel_size = self.conv_kernels[layer_index],\n",
        "        strides = self.conv_strides[layer_index],\n",
        "        padding = \"same\",\n",
        "        name = f\"encoder_conv_layer_{layer_number}\"\n",
        "    )\n",
        "    x = conv_layer(x)\n",
        "    x = ReLU(name=f\"encoder_relu_{layer_number}\")(x)\n",
        "    x = BatchNormalization(name=f\"encoder_bn_{layer_number}\")(x)\n",
        "    return x\n",
        "\n",
        "  #-------------LATTENT SPACE-------------#\n",
        "  def _add_bottleneck(self, x):\n",
        "    \"\"\"Flatten data and add bottleneck with Gaussian sampling (Dense layer)\"\"\"\n",
        "    self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
        "    x = Flatten()(x)\n",
        "    self.mu = Dense(self.latent_space_dim,name=\"mu\")(x)\n",
        "    self.log_variance = Dense(self.latent_space_dim,\n",
        "                              name=\"log_variance\")(x)\n",
        "    \n",
        "    def sample_point_from_normal_distribution(args):\n",
        "      mu, log_variance = args\n",
        "      epsilon = K.random_normal(shape=K.shape(self.mu), mean=0., stddev=1.)\n",
        "      sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
        "\n",
        "      return sampled_point\n",
        "\n",
        "    x = Lambda(sample_point_from_normal_distribution, \n",
        "              name=\"encoder_output\")([self.mu, self.log_variance])\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szGePKqqV8jq"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(x_train, learning_rate, batch_size, epochs):\n",
        "    vae_f0 = VAE(\n",
        "        input_shape=(512, 64, 1),\n",
        "        conv_filters=(512, 256, 128, 64, 32),\n",
        "        conv_kernels=(3, 3, 3, 3, 3),\n",
        "        conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "        latent_space_dim=vector_dimension,\n",
        "        foi=\"f0\"\n",
        "    )\n",
        "    vae_formants = VAE(\n",
        "        input_shape=(512, 64, 1),\n",
        "        conv_filters=(512, 256, 128, 64, 32),\n",
        "        conv_kernels=(3, 3, 3, 3, 3),\n",
        "        conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "        latent_space_dim=vector_dimension,\n",
        "        foi=\"formants\"\n",
        "    )\n",
        "    vae_f0.summary()\n",
        "    vae_f0.compile(learning_rate)\n",
        "\n",
        "    vae_formants.summary()\n",
        "    vae_formants.compile(learning_rate)\n",
        "\n",
        "    sum = tf.keras.layers.add([vae_f0.output, vae_formants.output])\n",
        "    #result = tf.keras.layers.Dense(4)(sum) \n",
        "\n",
        "    model = tf.keras.models.Model(inputs = x_train, outputs = sum)\n",
        "\n",
        "    model.train(x_train, batch_size, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'VAE' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/t6/hlfm94q93sz7p8x7djsrs8nh0000gp/T/ipykernel_2717/2872145566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vae_f0 = VAE(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mconv_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mconv_kernels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconv_strides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'VAE' is not defined"
          ]
        }
      ],
      "source": [
        "vae_f0 = VAE(\n",
        "    input_shape=(512, 64, 1),\n",
        "    conv_filters=(512, 256, 128, 64, 32),\n",
        "    conv_kernels=(3, 3, 3, 3, 3),\n",
        "    conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "    latent_space_dim=vector_dimension,\n",
        "    foi=\"f0\"\n",
        ")\n",
        "vae_formants = VAE(\n",
        "    input_shape=(512, 64, 1),\n",
        "    conv_filters=(512, 256, 128, 64, 32),\n",
        "    conv_kernels=(3, 3, 3, 3, 3),\n",
        "    conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "    latent_space_dim=vector_dimension,\n",
        "    foi=\"formants\"\n",
        ")\n",
        "vae_f0.summary()\n",
        "vae_f0.compile(learning_rate)\n",
        "\n",
        "vae_formants.summary()\n",
        "vae_formants.compile(learning_rate)\n",
        "\n",
        "sum = tf.keras.layers.add([vae_f0.output, vae_formants.output])\n",
        "result = tf.keras.layers.Dense(4)(sum)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=x_train, outputs=result)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtgR1jchNMw2"
      },
      "outputs": [],
      "source": [
        "def train(x_train, learning_rate, batch_size, epochs):\n",
        "    vae_f0 = VAE(\n",
        "        input_shape=(512, 64, 1),\n",
        "        conv_filters=(512, 256, 128, 64, 32),\n",
        "        conv_kernels=(3, 3, 3, 3, 3),\n",
        "        conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "        latent_space_dim=vector_dimension,\n",
        "        foi=\"f0\"\n",
        "    )\n",
        "    vae_formants = VAE(\n",
        "        input_shape=(512, 64, 1),\n",
        "        conv_filters=(512, 256, 128, 64, 32),\n",
        "        conv_kernels=(3, 3, 3, 3, 3),\n",
        "        conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "        latent_space_dim=vector_dimension,\n",
        "        foi=\"formants\"\n",
        "    )\n",
        "\n",
        "    vae_f0.summary()\n",
        "    vae_f0.compile(learning_rate)\n",
        "\n",
        "    vae_formants.summary()\n",
        "    vae_formants.compile(learning_rate)\n",
        "\n",
        "    #vae_total = tf.keras.layers.Add()([vae_f0.output, vae_f0.output])\n",
        "    \n",
        "    class CustomLayer(tf.keras.layers.Layer):\n",
        "\n",
        "        def vae_loss(self, x_train, z_decoded):\n",
        "            x_train = K.flatten(x_train)\n",
        "            z_decoded = K.flatten(z_decoded)\n",
        "\n",
        "            recon_loss = tf.keras.metrics.binary_crossentropy(x_train, z_decoded)\n",
        "\n",
        "            return K.mean(recon_loss)\n",
        "\n",
        "        # add custom loss to the class\n",
        "        def call(self, inputs):\n",
        "            x = inputs[0]\n",
        "            z_decoded = inputs[1]\n",
        "            loss = self.vae_loss(x, z_decoded)\n",
        "            self.add_loss(loss, inputs=inputs)\n",
        "            return x\n",
        "\n",
        "\n",
        "    vae_f0.train(x_train, batch_size, 1)\n",
        "    vae_formants.train(x_train, batch_size, 1)\n",
        "\n",
        "    vae_total = CustomLayer()([x_train, vae_f0.output + vae_formants.output])\n",
        "    recon_MSE = vae_total.vae_loss()\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        new_vae_f0 = VAE(\n",
        "            input_shape=(512, 64, 1),\n",
        "            conv_filters=(512, 256, 128, 64, 32),\n",
        "            conv_kernels=(3, 3, 3, 3, 3),\n",
        "            conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "            latent_space_dim=vector_dimension,\n",
        "            total_MSE=recon_MSE\n",
        "        )\n",
        "        new_vae_formants = VAE(\n",
        "            input_shape=(512, 64, 1),\n",
        "            conv_filters=(512, 256, 128, 64, 32),\n",
        "            conv_kernels=(3, 3, 3, 3, 3),\n",
        "            conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "            latent_space_dim=vector_dimension,\n",
        "            total_MSE=recon_MSE\n",
        "        )\n",
        "\n",
        "        new_vae_f0.set_weights(vae_f0.get_weights())\n",
        "        new_vae_formants.set_weights(vae_formants.get_weights())\n",
        "\n",
        "        new_vae_f0.train(x_train, batch_size, 1)\n",
        "        new_vae_formants.train(x_train, batch_size, 1)\n",
        "\n",
        "        vae_f0 = new_vae_f0\n",
        "        vae_formants = new_vae_formants\n",
        "\n",
        "        vae_total = CustomLayer()([x_train, vae_f0.output + vae_f0.output])\n",
        "        recon_MSE = vae_total.vae_loss()\n",
        "\n",
        "    return vae_total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'soundfile'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/t6/hlfm94q93sz7p8x7djsrs8nh0000gp/T/ipykernel_5834/1340446472.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/Documents/GitHub/VAE-Speech-Decomposition/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msoundfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'soundfile'"
          ]
        }
      ],
      "source": [
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Task Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(x_train, learning_rate, batch_size, epochs):\n",
        "\n",
        "    vae_f0 = VAE(\n",
        "        input_shape=(512, 64, 1),\n",
        "        conv_filters=(512, 256, 128, 64, 32),\n",
        "        conv_kernels=(3, 3, 3, 3, 3),\n",
        "        conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "        latent_space_dim=vector_dimension,\n",
        "        foi=\"f0\"\n",
        "    )\n",
        "    vae_formants = VAE(\n",
        "        input_shape=(512, 64, 1),\n",
        "        conv_filters=(512, 256, 128, 64, 32),\n",
        "        conv_kernels=(3, 3, 3, 3, 3),\n",
        "        conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "        latent_space_dim=vector_dimension,\n",
        "        foi=\"formants\"\n",
        "    )\n",
        "\n",
        "    #vae_f0.summary()\n",
        "    vae_f0.compile(learning_rate)\n",
        "\n",
        "    #vae_formants.summary()\n",
        "    vae_formants.compile(learning_rate)\n",
        "\n",
        "    x = tf.Variable(x_train, name=\"Patterns\", dtype=\"float32\")\n",
        "\n",
        "    f0_target = tf.Variable(f0_array(x, \"male\"), name=\"f0_target\", dtype=\"variant\")\n",
        "    f0_predicted = tf.Varaible(f0_array(vae_f0.predict(x)), name=\"f0_predicted\", dtype=\"variant\")\n",
        "    f0_Loss = tf.nn.l2_loss(f0_predicted-f0_target)\n",
        "\n",
        "    formants_target = tf.Variable(formants_array(x, \"male\"), name=\"formants_target\", dtype=\"variant\")\n",
        "    formants_predicted = tf.Varaible(formants_array(vae_formants.predict(x)), name=\"formants_predicted\", dtype=\"variant\")\n",
        "    formants_Loss = tf.nn.l2_loss(formants_predicted-formants_target)\n",
        "\n",
        "\n",
        "    Joint_Loss = f0_Loss + formants_Loss + tf.nn.l2_loss(x - (vae_f0.predict(x) + vae_formants.predict(x)))\n",
        "\n",
        "    Optimiser = tf.train.AdamOptimizer().minimize(Joint_Loss)\n",
        "    # f0_op = tf.train.AdamOptimizer().minimize(f0_Loss)\n",
        "    # formants_op = tf.train.AdamOptimizer().minimize(formants_Loss)\n",
        "\n",
        "    with tf.Session() as sesh:\n",
        "        sesh.run(tf.initialize_all_variables())\n",
        "        _, Joint_Loss = sesh.run([Optimiser, Joint_Loss])\n",
        "        print(Joint_Loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_ecB0kimMb"
      },
      "outputs": [],
      "source": [
        "def get_time_stamp():\n",
        "  secondsSinceEpoch = time.time()\n",
        "  timeObj = time.localtime(secondsSinceEpoch)\n",
        "  x = ('%d_%d_%d_%d%d' % (timeObj.tm_mday, timeObj.tm_mon, timeObj.tm_year, timeObj.tm_hour, timeObj.tm_min))\n",
        "  return x\n",
        "\n",
        "current_time = get_time_stamp()\n",
        "\n",
        "model_name = \"simple_vae\" #@param {type:\"string\"}\n",
        "save_dir = \"/content/drive/My Drive/vae-speech/\" #@param {type:\"string\"}\n",
        "\n",
        "print(x_train.shape)\n",
        "vae = train(x_train, learning_rate, batch_size, num_epochs_to_train)\n",
        "\n",
        "vae.save(f\"{save_dir}{model_name}_{current_time}__w{shape}sec_z{vector_dimension}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoWQX0P6vJX4"
      },
      "outputs": [],
      "source": [
        "loss = vae.hist.history['loss']\n",
        "\n",
        "epochs = range(num_epochs_to_train)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'g', label='Training loss')\n",
        "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuNxV9W0PndJ"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3X3p1YhPSfG"
      },
      "outputs": [],
      "source": [
        "def plot_specgram(spec, sample_rate, title=\"Spectrogram\", xlim=None):\n",
        "  spec = np.reshape(spec, (512, 64))\n",
        "  num_freq, num_frames = spec.shape\n",
        "  time_axis = np.arange(0, num_frames) / sample_rate\n",
        "  freq_axis = np.arange(0, num_freq) * sample_rate/2/num_freq\n",
        "  figure, axes = plt.subplots(1, 1)\n",
        "  axes.pcolormesh(time_axis, freq_axis, spec[:,:], cmap='viridis')\n",
        "  axes.set_xlim(xlim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiS_Kb6eOrSE"
      },
      "outputs": [],
      "source": [
        "# dir = '/content/drive/MyDrive/vae-speechsimple_vae_2_3_2022_165__w1sec_z64/'\n",
        "\n",
        "# ae_save = tf.keras.models.load_model(dir+'weights.h5')\n",
        "\n",
        "decoded_specgram = vae.model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEMLJxIOQZoT"
      },
      "outputs": [],
      "source": [
        "plot_specgram(x_test[0], sampling_rate)\n",
        "plot_specgram(decoded_specgram[0], sampling_rate)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
