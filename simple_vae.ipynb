{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhMtBKfvVtBa"
      },
      "source": [
        "# Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNDZuTKn2wdI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "!pip install tensorflow-gpu==2.3.1\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda, Cropping2D, ZeroPadding2D\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjAK7LB3VyVC"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN3x7QtB4DUf"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/My Drive/vae-speech'\n",
        "\n",
        "data = np.load(dir + '/male.npy', allow_pickle=True)\n",
        "\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWpgUQrkV05O"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gn1sVgEP7G4"
      },
      "outputs": [],
      "source": [
        "def update_dimension():\n",
        "  new_data = []\n",
        "\n",
        "  for idx, wav in enumerate(data):\n",
        "    d = np.delete(data[idx], -1, axis=0)\n",
        "    z = np.zeros((len(d), 1))\n",
        "    d = np.append(d, z, axis = 1)\n",
        "    new_data.append(d)\n",
        "\n",
        "  return np.array(new_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Byx_Wt9VTRO"
      },
      "outputs": [],
      "source": [
        "nd = update_dimension()\n",
        "n = len(nd)\n",
        "\n",
        "#reshape the data\n",
        "nd = np.reshape(nd, (len(nd), 512, 64, 1))\n",
        "\n",
        "\n",
        "x_train = nd[:int(0.6*n)]\n",
        "x_test = nd[int(-0.4*n):]\n",
        "\n",
        "print(\"new dimension: \" + str(nd.shape))\n",
        "print(\"training size: \" + str(len(x_train)))\n",
        "print(\"testing size: \" + str(len(x_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Auvg8DDsV44M"
      },
      "source": [
        "# Model generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ohoppyrKAJk"
      },
      "outputs": [],
      "source": [
        "shape = 1\n",
        "\n",
        "learning_rate = 0.001 #@param {type:\"raw\"}\n",
        "num_epochs_to_train = 30 #@param {type:\"integer\"}\n",
        "batch_size = 32 #@param {type:\"integer\"}\n",
        "vector_dimension = 64 #@param {type:\"integer\"}\n",
        "\n",
        "sampling_rate = 16000 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VAE:\n",
        "  def __init__(self,\n",
        "               input_shape, #shape of the input data\n",
        "               conv_filters, #convolutional network filters\n",
        "               conv_kernels, #convNet kernel size\n",
        "               conv_strides, #convNet strides\n",
        "               latent_space_dim):\n",
        "    self.input_shape = input_shape # (513, 63)\n",
        "    self.conv_filters = conv_filters # is a list for each layer, i.e. [2, 4, 8]\n",
        "    self.conv_kernels = conv_kernels # list of kernels per layer, [1, 2, 3]\n",
        "    self.conv_strides = conv_strides # stride for each filter [1, 2, 2], note: 2 means you are downsampling the data in half\n",
        "    self.latent_space_dim = latent_space_dim # how many neurons on bottleneck\n",
        "    self.reconstruction_loss_weight = 1000000\n",
        "\n",
        "    self.encoder = None\n",
        "    self.decoder = None\n",
        "    self.model = None\n",
        "    self.hist = None\n",
        "\n",
        "    self._num_conv_layers = len(conv_filters)\n",
        "    self._shape_before_bottleneck = None\n",
        "    self._model_input = None\n",
        "\n",
        "    self._build()\n",
        "\n",
        "  def summary(self):\n",
        "    self.encoder.summary()\n",
        "    print(\"\\n\")\n",
        "    self.decoder.summary()\n",
        "    print(\"\\n\")\n",
        "    self.model.summary()\n",
        "\n",
        "  def _build(self):\n",
        "    self._build_encoder()\n",
        "    self._build_decoder()\n",
        "    self._build_autoencoder()\n",
        "\n",
        "  def compile(self, learning_rate=0.0001):\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    self.model.compile(optimizer=optimizer, loss=self._calculate_combined_loss,\n",
        "                      metrics=[self._calculate_reconstruction_loss, self._calculate_kl_loss])\n",
        "  \n",
        "  def train(self, x_train, batch_size, num_epochs):\n",
        "    self.hist= self.model.fit(x_train,\n",
        "                              x_train,\n",
        "                              batch_size=batch_size,\n",
        "                              epochs=num_epochs,\n",
        "                              shuffle=True)\n",
        "\n",
        "  def _calculate_combined_loss(self, y_target, y_predicted):\n",
        "    reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
        "    kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
        "    foi_loss = self._calculate_foi_loss(y_target, y_predicted)\n",
        "    combined_loss = self.reconstruction_loss_weight * reconstruction_loss + kl_loss\n",
        "    return combined_loss\n",
        "  \n",
        "  def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
        "    error = y_target - y_predicted\n",
        "    reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "    return reconstruction_loss\n",
        "\n",
        "  def _calculate_kl_loss(self, y_target, y_predicted):\n",
        "    kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) -\n",
        "                          K.exp(self.log_variance), axis =1)\n",
        "    return kl_loss\n",
        "\n",
        "  def _calculate_foi_loss (self, formant_target, formant_predicted):   # foi: feature of interest (f0/formants)\n",
        "    error = formant_target - formant_predicted\n",
        "    formant_loss = K.mean(K.square(error), axis=[1, 2, 3])  # TODO: axis?\n",
        "    return formant_loss \n",
        "  \n",
        "  def save(self, save_folder):\n",
        "    self._create_folder_if_it_doesnt_exist(save_folder)\n",
        "    self._save_parameters(save_folder)\n",
        "    self._save_weights(save_folder)\n",
        "\n",
        "  def _create_folder_if_it_doesnt_exist(self, folder):\n",
        "      if not os.path.exists(folder):\n",
        "          os.makedirs(folder)\n",
        "\n",
        "  def _save_parameters(self, save_folder):\n",
        "      parameters = [\n",
        "          self.input_shape,\n",
        "          self.conv_filters,\n",
        "          self.conv_kernels,\n",
        "          self.conv_strides,\n",
        "          self.latent_space_dim\n",
        "      ]\n",
        "      save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "      with open(save_path, \"wb\") as f:\n",
        "          pickle.dump(parameters, f)\n",
        "\n",
        "  def _save_weights(self, save_folder):\n",
        "      save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "      self.model.save_weights(save_path)\n",
        "\n",
        "  #----------------FULL MODEL-----------------#\n",
        "  def _build_autoencoder(self):\n",
        "    model_input = self._model_input\n",
        "    model_output = self.decoder(self.encoder(model_input))\n",
        "    self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
        "\n",
        "  #----------------DECODER-----------------#\n",
        "  def _build_decoder(self):\n",
        "    decoder_input = self._add_decoder_input()\n",
        "    dense_layer = self._add_dense_layer(decoder_input)\n",
        "    reshape_layer = self._add_reshape_layer(dense_layer)\n",
        "    conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
        "    decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
        "    self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "\n",
        "  def _add_decoder_input(self):\n",
        "    return Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
        "\n",
        "  def _add_dense_layer(self, decoder_input):\n",
        "    num_neurons = np.prod(self._shape_before_bottleneck) # [ 1, 2, 4] -> 8\n",
        "    dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
        "    return dense_layer\n",
        "\n",
        "  def _add_reshape_layer(self, dense_layer):\n",
        "    return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
        "\n",
        "  def _add_conv_transpose_layers(self, x):\n",
        "    for layer_index in reversed(range(1, self._num_conv_layers)):\n",
        "      x = self._add_conv_transpose_layer(layer_index, x)\n",
        "    return x\n",
        "\n",
        "  def _add_conv_transpose_layer(self, layer_index, x):\n",
        "    layer_num = self._num_conv_layers - layer_index\n",
        "    conv_transpose_layer = Conv2DTranspose(\n",
        "        filters=self.conv_filters[layer_index],\n",
        "        kernel_size = self.conv_kernels[layer_index],\n",
        "        strides = self.conv_strides[layer_index],\n",
        "        padding = \"same\",\n",
        "        name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
        "    )\n",
        "    x = conv_transpose_layer(x)\n",
        "    x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
        "    x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
        "    return x\n",
        "\n",
        "  def _add_decoder_output(self, x):\n",
        "    conv_transpose_layer = Conv2DTranspose(\n",
        "        filters = 1,\n",
        "        kernel_size = self.conv_kernels[0],\n",
        "        strides = self.conv_strides[0],\n",
        "        padding = \"same\",\n",
        "        name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
        "    )\n",
        "    x = conv_transpose_layer(x)\n",
        "    output_layer = Activation(\"sigmoid\", name=\"sigmoid_output_layer\")(x)\n",
        "    return output_layer\n",
        "\n",
        "  #----------------ENCODER-----------------#\n",
        "  def _build_encoder(self):\n",
        "    encoder_input = self._add_encoder_input()\n",
        "    # x = Cropping2D(cropping=((1, 0), (0,0)))(encoder_input)\n",
        "    # x = ZeroPadding2D(padding=((0, 0), (1,0)))(x)\n",
        "    conv_layers = self._add_conv_layers(encoder_input)\n",
        "    bottleneck =  self._add_bottleneck(conv_layers)\n",
        "    self._model_input = encoder_input\n",
        "    self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
        "\n",
        "  def _add_encoder_input(self):\n",
        "    return Input(shape=self.input_shape, name=\"encoder_input\")\n",
        "\n",
        "  def _add_conv_layers(self, encoder_input):\n",
        "    \"\"\"Creates all convolutional blocks in encoder\"\"\"\n",
        "    x = encoder_input\n",
        "    for layer_index in range(self._num_conv_layers):\n",
        "      x = self._add_conv_layer(layer_index, x)\n",
        "    return x\n",
        "  \n",
        "  def _add_conv_layer(self, layer_index, x):\n",
        "    \"\"\"\n",
        "    Adds a convolutional block to a graph of layers, consisting\n",
        "    of Conv 2d + ReLu activation + batch normalization.\n",
        "    \"\"\"\n",
        "    layer_number = layer_index + 1\n",
        "    conv_layer = Conv2D(\n",
        "        filters= self.conv_filters[layer_index],\n",
        "        kernel_size = self.conv_kernels[layer_index],\n",
        "        strides = self.conv_strides[layer_index],\n",
        "        padding = \"same\",\n",
        "        name = f\"encoder_conv_layer_{layer_number}\"\n",
        "    )\n",
        "    x = conv_layer(x)\n",
        "    x = ReLU(name=f\"encoder_relu_{layer_number}\")(x)\n",
        "    x = BatchNormalization(name=f\"encoder_bn_{layer_number}\")(x)\n",
        "    return x\n",
        "\n",
        "  #-------------LATTENT SPACE-------------#\n",
        "  def _add_bottleneck(self, x):\n",
        "    \"\"\"Flatten data and add bottleneck with Gaussian sampling (Dense layer)\"\"\"\n",
        "    self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
        "    x = Flatten()(x)\n",
        "    self.mu = Dense(self.latent_space_dim,name=\"mu\")(x)\n",
        "    self.log_variance = Dense(self.latent_space_dim,\n",
        "                              name=\"log_variance\")(x)\n",
        "    \n",
        "    def sample_point_from_normal_distribution(args):\n",
        "      mu, log_variance = args\n",
        "      epsilon = K.random_normal(shape=K.shape(self.mu), mean=0., stddev=1.)\n",
        "      sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
        "\n",
        "      return sampled_point\n",
        "\n",
        "    x = Lambda(sample_point_from_normal_distribution, \n",
        "              name=\"encoder_output\")([self.mu, self.log_variance])\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szGePKqqV8jq"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtgR1jchNMw2"
      },
      "outputs": [],
      "source": [
        "def train(x_train, learning_rate, batch_size, epochs):\n",
        "    vae_f0 = VAE(\n",
        "        input_shape=(512, 64, 1),\n",
        "        conv_filters=(512, 256, 128, 64, 32),\n",
        "        conv_kernels=(3, 3, 3, 3, 3),\n",
        "        conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "        latent_space_dim=vector_dimension\n",
        "    )\n",
        "    vae_formants = VAE(\n",
        "        input_shape=(512, 64, 1),\n",
        "        conv_filters=(512, 256, 128, 64, 32),\n",
        "        conv_kernels=(3, 3, 3, 3, 3),\n",
        "        conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "        latent_space_dim=vector_dimension\n",
        "    )\n",
        "\n",
        "    vae_f0.summary()\n",
        "    vae_f0.compile(learning_rate)\n",
        "\n",
        "    vae_formants.summary()\n",
        "    vae_formants.compile(learning_rate)\n",
        "\n",
        "    #vae_total = tf.keras.layers.Add()([vae_f0.output, vae_f0.output])\n",
        "    \n",
        "    class CustomLayer(tf.keras.layers.Layer):\n",
        "\n",
        "        def vae_loss(self, x_train, z_decoded):\n",
        "            x = K.flatten(x)\n",
        "            z_decoded = K.flatten(z_decoded)\n",
        "\n",
        "            recon_loss = tf.keras.metrics.binary_crossentropy(x, x_train)\n",
        "\n",
        "            return K.mean(recon_loss)\n",
        "\n",
        "        # add custom loss to the class\n",
        "        def call(self, inputs):\n",
        "            x = inputs[0]\n",
        "            z_decoded = inputs[1]\n",
        "            loss = self.vae_loss(x, z_decoded)\n",
        "            self.add_loss(loss, inputs=inputs)\n",
        "            return x\n",
        "\n",
        "    vae_total = CustomLayer()([x_train, vae_f0.output + vae_f0.output])\n",
        "    recon_MSE = vae_total.vae_loss()\n",
        "\n",
        "    vae_f0.train(x_train, batch_size, epochs, recon_MSE)\n",
        "    vae_formants.train(x_train, batch_size, epochs, recon_MSE)\n",
        "    return vae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_ecB0kimMb"
      },
      "outputs": [],
      "source": [
        "def get_time_stamp():\n",
        "  secondsSinceEpoch = time.time()\n",
        "  timeObj = time.localtime(secondsSinceEpoch)\n",
        "  x = ('%d_%d_%d_%d%d' % (timeObj.tm_mday, timeObj.tm_mon, timeObj.tm_year, timeObj.tm_hour, timeObj.tm_min))\n",
        "  return x\n",
        "\n",
        "current_time = get_time_stamp()\n",
        "\n",
        "model_name = \"simple_vae\" #@param {type:\"string\"}\n",
        "save_dir = \"/content/drive/My Drive/vae-speech/\" #@param {type:\"string\"}\n",
        "\n",
        "print(x_train.shape)\n",
        "vae = train(x_train, learning_rate, batch_size, num_epochs_to_train)\n",
        "\n",
        "vae.save(f\"{save_dir}{model_name}_{current_time}__w{shape}sec_z{vector_dimension}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoWQX0P6vJX4"
      },
      "outputs": [],
      "source": [
        "loss = vae.hist.history['loss']\n",
        "\n",
        "epochs = range(num_epochs_to_train)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'g', label='Training loss')\n",
        "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuNxV9W0PndJ"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3X3p1YhPSfG"
      },
      "outputs": [],
      "source": [
        "def plot_specgram(spec, sample_rate, title=\"Spectrogram\", xlim=None):\n",
        "  spec = np.reshape(spec, (512, 64))\n",
        "  num_freq, num_frames = spec.shape\n",
        "  time_axis = np.arange(0, num_frames) / sample_rate\n",
        "  freq_axis = np.arange(0, num_freq) * sample_rate/2/num_freq\n",
        "  figure, axes = plt.subplots(1, 1)\n",
        "  axes.pcolormesh(time_axis, freq_axis, spec[:,:], cmap='viridis')\n",
        "  axes.set_xlim(xlim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiS_Kb6eOrSE"
      },
      "outputs": [],
      "source": [
        "# dir = '/content/drive/MyDrive/vae-speechsimple_vae_2_3_2022_165__w1sec_z64/'\n",
        "\n",
        "# ae_save = tf.keras.models.load_model(dir+'weights.h5')\n",
        "\n",
        "decoded_specgram = vae.model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEMLJxIOQZoT"
      },
      "outputs": [],
      "source": [
        "plot_specgram(x_test[0], sampling_rate)\n",
        "plot_specgram(decoded_specgram[0], sampling_rate)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
