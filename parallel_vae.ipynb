{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parallel_vae.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import dependencies"
      ],
      "metadata": {
        "id": "xhMtBKfvVtBa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TNDZuTKn2wdI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7168fbe-6292-40fb-cfd9-6f7732cd2e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu==2.3.1\n",
            "  Downloading tensorflow_gpu-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 24 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (3.17.3)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (2.8.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (0.2.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (1.13.3)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (1.44.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (0.37.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1) (3.2.0)\n",
            "Installing collected packages: numpy, tensorflow-estimator, h5py, gast, tensorflow-gpu\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.0+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.1 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 numpy-1.18.5 tensorflow-estimator-2.3.0 tensorflow-gpu-2.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "!pip install tensorflow-gpu==2.3.1\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda, Dropout\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "UjAK7LB3VyVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_usVkLKL8CRc",
        "outputId": "5a249d1e-a473-4a09-a6eb-d300b0b1a5af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/My Drive/vae-speech'\n",
        "\n",
        "spec = np.load(dir + '/male_spec.npy', allow_pickle=True)\n",
        "frt = np.load(dir + '/male_frt.npy', allow_pickle=True)\n",
        "f0 = np.load(dir + '/male_f0.npy', allow_pickle=True)\n",
        "\n",
        "print(spec.shape)\n",
        "print(frt.shape)\n",
        "print(f0.shape)"
      ],
      "metadata": {
        "id": "pN3x7QtB4DUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8aca4e-f447-43f6-c6aa-933ed997443f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2489, 513, 63)\n",
            "(2489, 438)\n",
            "(2489, 197)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "SWpgUQrkV05O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_dimension():\n",
        "  new_spec = []\n",
        "\n",
        "  for idx, wav in enumerate(spec):\n",
        "    d = np.delete(spec[idx], -1, axis=0)\n",
        "    z = np.zeros((len(d), 1))\n",
        "    d = np.append(d, z, axis = 1)\n",
        "    new_spec.append(d)\n",
        "\n",
        "  return np.array(new_spec)"
      ],
      "metadata": {
        "id": "8Gn1sVgEP7G4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nd = update_dimension()\n",
        "n = len(nd)\n",
        "\n",
        "#reshape the data\n",
        "nd = np.reshape(nd, (n, 512, 64, 1))\n",
        "nf = np.reshape(frt, (n, len(frt[0]), 1))\n",
        "ns = np.reshape(f0, (n, len(f0[0]), 1))\n",
        "\n",
        "x_train_spec = nd[:int(0.6*n)]\n",
        "x_test_spec = nd[int(-0.4*n):]\n",
        "\n",
        "x_train_frt = nf[:int(0.6*n)]\n",
        "x_test_frt = nf[int(-0.4*n):]\n",
        "\n",
        "x_train_f0 = ns[:int(0.6*n)]\n",
        "x_test_f0 = ns[int(-0.4*n):]\n",
        "\n",
        "print(\"new spec dimension: \" + str(x_train_spec.shape))\n",
        "print(\"new f0 dimension: \" + str(x_train_f0.shape))\n",
        "print(\"new formant dimension: \" + str(x_train_frt.shape))\n",
        "print(\"training size: \" + str(len(x_train_spec)))\n",
        "print(\"testing size: \" + str(len(x_test_spec)))"
      ],
      "metadata": {
        "id": "4Byx_Wt9VTRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea3b471-fcc1-40dc-b634-1f401959d7d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new spec dimension: (1493, 512, 64, 1)\n",
            "new f0 dimension: (1493, 197, 1)\n",
            "new formant dimension: (1493, 438, 1)\n",
            "training size: 1493\n",
            "testing size: 995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model generation"
      ],
      "metadata": {
        "id": "Auvg8DDsV44M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE"
      ],
      "metadata": {
        "id": "t0mH7tNuVbbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape = 1\n",
        "\n",
        "learning_rate = 0.001 #@param {type:\"raw\"}\n",
        "num_epochs_to_train = 30 #@param {type:\"integer\"}\n",
        "batch_size = 32 #@param {type:\"integer\"}\n",
        "vector_dimension = 64 #@param {type:\"integer\"}\n",
        "\n",
        "sampling_rate = 16000 #@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "6ohoppyrKAJk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE:\n",
        "  def __init__(self,\n",
        "               model_type,\n",
        "               foi_input,\n",
        "               model_input,\n",
        "               foi_shape,\n",
        "               conv_filters, #convolutional network filters\n",
        "               conv_kernels, #convNet kernel size\n",
        "               conv_strides, #convNet strides\n",
        "               latent_space_dim):\n",
        "    self.model_type = model_type\n",
        "    self.foi_input = foi_input\n",
        "    self.model_input = model_input\n",
        "    self.foi_shape = foi_shape\n",
        "    self.conv_filters = conv_filters # is a list for each layer, i.e. [2, 4, 8]\n",
        "    self.conv_kernels = conv_kernels # list of kernels per layer, [1, 2, 3]\n",
        "    self.conv_strides = conv_strides # stride for each filter [1, 2, 2], note: 2 means you are downsampling the data in half\n",
        "    self.latent_space_dim = latent_space_dim # how many neurons on bottleneck\n",
        "    self.loss_weight = 1000000\n",
        "\n",
        "    self.mlp = None\n",
        "    self.encoder = None\n",
        "    self.decoder = None\n",
        "    self.model = None\n",
        "    self.hist = None\n",
        "\n",
        "    self.vae_output = None\n",
        "    self.mlp_output = None\n",
        "\n",
        "    self._num_conv_layers = len(conv_filters)\n",
        "    self._shape_before_bottleneck = None\n",
        "\n",
        "    self._build()\n",
        "\n",
        "  def summary(self):\n",
        "    self.encoder.summary()\n",
        "    print(\"\\n\")\n",
        "    self.decoder.summary()\n",
        "    print(\"\\n\")\n",
        "    self.model.summary()\n",
        "\n",
        "  def _build(self):\n",
        "    self._build_encoder()\n",
        "    self._build_decoder()\n",
        "    self._build_mlp()\n",
        "    self._build_autoencoder()\n",
        "    self._add_model_loss()\n",
        "    self._add_model_optimizer()\n",
        "\n",
        "  def _add_model_optimizer(self):\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    self.model.compile(optimizer=optimizer, metrics=[self._calculate_kl_loss, \n",
        "                                                    self._calculate_reconstruction_loss, \n",
        "                                                    self._calculate_foi_loss])\n",
        "\n",
        "  def _add_model_loss(self):\n",
        "    kl_loss = self._calculate_kl_loss(self.model_input, self.vae_output)\n",
        "    reconstruction_loss = self._calculate_reconstruction_loss(self.model_input, self.vae_output)\n",
        "    feature_loss = self._calculate_foi_loss(self.foi_input, self.mlp_output)\n",
        "\n",
        "    combined_loss = kl_loss + self.loss_weight * (reconstruction_loss + feature_loss)\n",
        "    self.model.add_loss(combined_loss)\n",
        "\n",
        "  def _calculate_foi_loss(self, y_target, y_predicted):\n",
        "    error = y_target - y_predicted\n",
        "    reconstruction_loss = K.mean(K.square(error), axis=0)\n",
        "    return reconstruction_loss\n",
        "  \n",
        "  def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
        "    error = y_target - y_predicted\n",
        "    reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "    return reconstruction_loss\n",
        "\n",
        "  def _calculate_kl_loss(self, y_target, y_predicted):\n",
        "    kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) -\n",
        "                          K.exp(self.log_variance), axis=1)\n",
        "    return kl_loss\n",
        "\n",
        "  #----------------FULL MODEL-----------------#\n",
        "  def _build_autoencoder(self):\n",
        "    model_input = self.model_input\n",
        "    feature_input = self.foi_input\n",
        "    self.vae_output = self.decoder(self.encoder(model_input))\n",
        "    self.mlp_output = self.mlp(self.encoder(model_input))\n",
        "    self.model = Model([model_input, feature_input], \n",
        "                       [self.vae_output, self.mlp_output], \n",
        "                       name=f\"{self.model_type}_autoencoder\")\n",
        "\n",
        "  #----------------MLP-----------------#\n",
        "  def _build_mlp(self):\n",
        "    mlp_input = self._add_mlp_input()\n",
        "    mlp_output = self._add_mlp_layers(mlp_input)\n",
        "    self.mlp = Model(mlp_input, mlp_output, name=f\"MLP_{self.model_type}_model\")\n",
        "\n",
        "  def _add_mlp_input(self):\n",
        "    return Input(shape=self.latent_space_dim, name=f\"{self.model_type}_mlp_input\")\n",
        "\n",
        "  def _add_mlp_layers(self, mlp_input):\n",
        "    num_neurons = np.prod(self._shape_before_bottleneck)\n",
        "    d_layer = Dense(num_neurons, activation='relu', name=f\"MLP_{self.model_type}_layer1\")(mlp_input)\n",
        "    o_layer = Dropout(0.45, name=f\"MLP_{self.model_type}_dropout1\")(d_layer)\n",
        "    d_layer = Dense(512, activation='relu', name=f\"MLP_{self.model_type}_layer2\")(o_layer)\n",
        "    o_layer = Dropout(0.45, name=f\"MLP_{self.model_type}_dropout2\")(d_layer)\n",
        "    d_layer = Dense(self.foi_shape, activation='softmax', name=f\"MLP_{self.model_type}_layer3\")(o_layer)\n",
        "    r_layer = Reshape((self.foi_shape, 1), name=f\"MLP_{self.model_type}_reshape\")(d_layer)\n",
        "    return r_layer\n",
        "\n",
        "  #----------------DECODER-----------------#\n",
        "  def _build_decoder(self):\n",
        "    decoder_input = self._add_decoder_input()\n",
        "    dense_layer = self._add_dense_layer(decoder_input)\n",
        "    reshape_layer = self._add_reshape_layer(dense_layer)\n",
        "    conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
        "    decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
        "    self.decoder = Model(decoder_input, decoder_output, name=f\"{self.model_type}_decoder\")\n",
        "\n",
        "  def _add_decoder_input(self):\n",
        "    return Input(shape=self.latent_space_dim, name=f\"{self.model_type}_decoder_input\")\n",
        "\n",
        "  def _add_dense_layer(self, decoder_input):\n",
        "    num_neurons = np.prod(self._shape_before_bottleneck) # [ 1, 2, 4] -> 8\n",
        "    dense_layer = Dense(num_neurons, name=f\"{self.model_type}_decoder_dense\")(decoder_input)\n",
        "    return dense_layer\n",
        "\n",
        "  def _add_reshape_layer(self, dense_layer):\n",
        "    return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
        "\n",
        "  def _add_conv_transpose_layers(self, x):\n",
        "    for layer_index in reversed(range(1, self._num_conv_layers)):\n",
        "      x = self._add_conv_transpose_layer(layer_index, x)\n",
        "    return x\n",
        "\n",
        "  def _add_conv_transpose_layer(self, layer_index, x):\n",
        "    layer_num = self._num_conv_layers - layer_index\n",
        "    conv_transpose_layer = Conv2DTranspose(\n",
        "        filters=self.conv_filters[layer_index],\n",
        "        kernel_size = self.conv_kernels[layer_index],\n",
        "        strides = self.conv_strides[layer_index],\n",
        "        padding = \"same\",\n",
        "        name=f\"{self.model_type}_decoder_conv_transpose_layer_{layer_num}\"\n",
        "    )\n",
        "    x = conv_transpose_layer(x)\n",
        "    x = ReLU(name=f\"{self.model_type}_decoder_relu_{layer_num}\")(x)\n",
        "    x = BatchNormalization(name=f\"{self.model_type}_decoder_bn_{layer_num}\")(x)\n",
        "    return x\n",
        "\n",
        "  def _add_decoder_output(self, x):\n",
        "    conv_transpose_layer = Conv2DTranspose(\n",
        "        filters = 1,\n",
        "        kernel_size = self.conv_kernels[0],\n",
        "        strides = self.conv_strides[0],\n",
        "        padding = \"same\",\n",
        "        name=f\"{self.model_type}decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
        "    )\n",
        "    x = conv_transpose_layer(x)\n",
        "    output_layer = Activation(\"sigmoid\", name=f\"{self.model_type}_sigmoid_output_layer\")(x)\n",
        "    return output_layer\n",
        "\n",
        "  #----------------ENCODER-----------------#\n",
        "  def _build_encoder(self):\n",
        "    conv_layers = self._add_conv_layers(self.model_input)\n",
        "    bottleneck =  self._add_bottleneck(conv_layers)\n",
        "    self.encoder = Model(self.model_input, bottleneck, name=f\"{self.model_type}_encoder\")\n",
        "\n",
        "  def _add_conv_layers(self, encoder_input):\n",
        "    \"\"\"Creates all convolutional blocks in encoder\"\"\"\n",
        "    x = encoder_input\n",
        "    for layer_index in range(self._num_conv_layers):\n",
        "      x = self._add_conv_layer(layer_index, x)\n",
        "    return x\n",
        "  \n",
        "  def _add_conv_layer(self, layer_index, x):\n",
        "    \"\"\"\n",
        "    Adds a convolutional block to a graph of layers, consisting\n",
        "    of Conv 2d + ReLu activation + batch normalization.\n",
        "    \"\"\"\n",
        "    layer_number = layer_index + 1\n",
        "    conv_layer = Conv2D(\n",
        "        filters= self.conv_filters[layer_index],\n",
        "        kernel_size = self.conv_kernels[layer_index],\n",
        "        strides = self.conv_strides[layer_index],\n",
        "        padding = \"same\",\n",
        "        name = f\"{self.model_type}_encoder_conv_layer_{layer_number}\"\n",
        "    )\n",
        "    x = conv_layer(x)\n",
        "    x = ReLU(name=f\"{self.model_type}_encoder_relu_{layer_number}\")(x)\n",
        "    x = BatchNormalization(name=f\"{self.model_type}_encoder_bn_{layer_number}\")(x)\n",
        "    return x\n",
        "\n",
        "  #-------------LATTENT SPACE-------------#\n",
        "  def _add_bottleneck(self, x):\n",
        "    \"\"\"Flatten data and add bottleneck with Gaussian sampling (Dense layer)\"\"\"\n",
        "    self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
        "    x = Flatten()(x)\n",
        "    self.mu = Dense(self.latent_space_dim,name=\"mu\")(x)\n",
        "    self.log_variance = Dense(self.latent_space_dim,\n",
        "                              name=f\"{self.model_type}_log_variance\")(x)\n",
        "    \n",
        "    def sample_point_from_normal_distribution(args):\n",
        "      mu, log_variance = args\n",
        "      epsilon = K.random_normal(shape=K.shape(self.mu), mean=0., stddev=1.)\n",
        "      sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
        "\n",
        "      return sampled_point\n",
        "\n",
        "    x = Lambda(sample_point_from_normal_distribution, \n",
        "              name=f\"{self.model_type}_encoder_output\")([self.mu, self.log_variance])\n",
        "    return x"
      ],
      "metadata": {
        "id": "DyQPJLkDK3Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "szGePKqqV8jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model(model_input, foi_input, foi_shape, model_type):\n",
        "  vae = VAE(\n",
        "      model_type = model_type,\n",
        "      foi_input = foi_input,\n",
        "      model_input = model_input,\n",
        "      foi_shape = foi_shape,\n",
        "      conv_filters=(512, 256, 128, 64, 32),\n",
        "      conv_kernels=(3, 3, 3, 3, 3),\n",
        "      conv_strides=(2, 2, 2, 2, (2,1)),\n",
        "      latent_space_dim = vector_dimension\n",
        "  )\n",
        "  return vae"
      ],
      "metadata": {
        "id": "FtgR1jchNMw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_reconstruction_loss(y_target, y_predicted):\n",
        "    error = y_target - y_predicted\n",
        "    reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "    return reconstruction_loss\n",
        "\n",
        "f0_input = Input(shape=(197, 1), name='f0_input')\n",
        "frt_input = Input(shape=(438, 1), name='formant_input')\n",
        "model_input = Input(shape=(512, 64, 1), name=\"model_input\")\n",
        "\n",
        "vae_f0 = generate_model(model_input, f0_input, 197, \"f0\")\n",
        "vae_frt = generate_model(model_input, frt_input, 438, \"frt\")\n",
        "\n",
        "model = Model(inputs=[model_input, f0_input, frt_input], outputs=[vae_f0.vae_output, vae_f0.vae_output])\n",
        "\n",
        "loss = calculate_reconstruction_loss(K.log(model_input), K.log(vae_f0.vae_output) + K.log(vae_frt.vae_output))\n",
        "model.add_loss(loss)\n",
        "\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, metrics=[calculate_reconstruction_loss])\n",
        "\n",
        "# tf.keras.utils.plot_model(model)"
      ],
      "metadata": {
        "id": "L7tin7Yi8cqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model)"
      ],
      "metadata": {
        "id": "FuF-Ffg1ae10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([x_train_spec, x_train_f0, x_train_frt],\n",
        "          batch_size=batch_size,\n",
        "          epochs=num_epochs_to_train)"
      ],
      "metadata": {
        "id": "JRalrQR4Dd-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_stamp():\n",
        "  secondsSinceEpoch = time.time()\n",
        "  timeObj = time.localtime(secondsSinceEpoch)\n",
        "  x = ('%d_%d_%d_%d%d' % (timeObj.tm_mday, timeObj.tm_mon, timeObj.tm_year, timeObj.tm_hour, timeObj.tm_min))\n",
        "  return x\n",
        "\n",
        "def save(model, save_folder):\n",
        "    create_folder_if_it_doesnt_exist(save_folder)\n",
        "    save_weights(model, save_folder)\n",
        "\n",
        "def create_folder_if_it_doesnt_exist(folder):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "def save_weights(model, save_folder):\n",
        "    save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "    model.save_weights(save_path)\n",
        "\n",
        "save_dir = '/content/drive/My Drive/vae-speech/'\n",
        "current_time = get_time_stamp()\n",
        "\n",
        "save(vae_f0.model, f\"{save_dir}f0_{current_time}_z{vector_dimension}\")\n",
        "save(vae_frt.model, f\"{save_dir}frt_{current_time}_z{vector_dimension}\")\n",
        "save(model, f\"{save_dir}full_{current_time}_z{vector_dimension}\")"
      ],
      "metadata": {
        "id": "0JK-oOZuZBL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_specgram(spec, sample_rate, title=\"Spectrogram\", xlim=None):\n",
        "  spec = np.reshape(spec, (512, 64))\n",
        "  num_freq, num_frames = spec.shape\n",
        "  time_axis = np.arange(0, num_frames) / sample_rate\n",
        "  freq_axis = np.arange(0, num_freq) * sample_rate/2/num_freq\n",
        "  figure, axes = plt.subplots(1, 1)\n",
        "  axes.pcolormesh(time_axis, freq_axis, spec[:,:], cmap='viridis')\n",
        "  axes.set_xlim(xlim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)\n",
        "\n",
        "decoded_specgram = model.predict([x_test_spec, x_test_f0, x_test_frt])\n",
        "\n",
        "plot_specgram(x_test_spec[0], sampling_rate)\n",
        "plot_specgram(decoded_specgram[0], sampling_rate)"
      ],
      "metadata": {
        "id": "HHVBBPg2ad1Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}