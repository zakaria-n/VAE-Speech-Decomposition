{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 15:52:47.728350: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-02 15:52:47.728373: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import IPython\n",
    "import time\n",
    "import librosa\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop=256               #hop size (window size = 4*hop)\n",
    "sr=16000             #sampling rate\n",
    "min_level_db=-100     #reference values to normalize data\n",
    "ref_level_db=20\n",
    "\n",
    "\n",
    "shape=128           #length of time axis of split specrograms         \n",
    "spec_split=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchaudio.transforms import MelScale, Spectrogram\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "specobj = Spectrogram(n_fft=4*hop, win_length=4*hop, hop_length=hop, pad=0, power=2, normalized=False)\n",
    "specfunc = specobj.forward\n",
    "\n",
    "def melspecfunc(waveform):\n",
    "  specgram = specfunc(waveform)\n",
    "  #mel_specgram = melfunc(specgram)\n",
    "  #return mel_specgram\n",
    "  return specgram\n",
    "\n",
    "def normalize(S):\n",
    "  return np.clip((((S - min_level_db) / -min_level_db)*2.)-1., -1, 1)\n",
    "\n",
    "def prep(wv, hop=192):\n",
    "  S = np.array(torch.squeeze(melspecfunc(torch.Tensor(wv).view(1,-1))).detach().cpu())\n",
    "  S = librosa.power_to_db(S)-ref_level_db\n",
    "  return normalize(S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate spectrograms from waveform array\n",
    "def tospec(data):\n",
    "  spectro = []\n",
    "  for awv in data:\n",
    "    spec = prep(awv)\n",
    "    spectro.append(spec)\n",
    "  return np.array(spectro, dtype=np.float32)\n",
    "    \n",
    "\n",
    "#Waveform array from path of folder containing wav files\n",
    "def audio_array(path):\n",
    "  ls = glob(f'{path}/*.wav')\n",
    "  adata = []\n",
    "  for i in range(len(ls)):\n",
    "    x, sr = tf.audio.decode_wav(tf.io.read_file(ls[i]), 1)\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "\n",
    "    time = 1\n",
    "    length = len(x)/sr\n",
    "\n",
    "    while time < length:\n",
    "      adata.append(x[(time-1)*sr:time*sr])\n",
    "      time += 1\n",
    "\n",
    "  return np.array(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 15:53:12.978135: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-02 15:53:12.978174: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (xps-ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-03-02 15:53:12.980074: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2489, 513, 63)\n"
     ]
    }
   ],
   "source": [
    "audio_directory = \"./Dataset/wav/male\"\n",
    "array_file = './Dataset/array/male'\n",
    "\n",
    "#get waveform array from folder containing wav files\n",
    "awv = audio_array(audio_directory)\n",
    "\n",
    "#get spectrogram array\n",
    "aspec = tospec(awv)\n",
    "\n",
    "print(aspec.shape)\n",
    "\n",
    "np.save(array_file,aspec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "  if src:\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", src)\n",
    "    print(\"-\" * 10)\n",
    "  if sample_rate:\n",
    "    print(\"Sample Rate:\", sample_rate)\n",
    "  print(\"Shape:\", tuple(waveform.shape))\n",
    "  print(\"Dtype:\", waveform.dtype)\n",
    "  print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "  print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "  print()\n",
    "  print(waveform)\n",
    "  print()\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "    axes[c].grid(True)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "    if ylim:\n",
    "      axes[c].set_ylim(ylim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_specgram_from_wave(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = np.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def play_audio(waveform, sample_rate):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  if num_channels == 1:\n",
    "    display(Audio(waveform[0], rate=sample_rate))\n",
    "  elif num_channels == 2:\n",
    "    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
    "  else:\n",
    "    raise ValueError(\"Waveform with more than 2 channels are not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "SAMPLE_WAV_SPEECH_PATH = \"./Dataset/wav/female/arctic_a0001.wav\"\n",
    "\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(SAMPLE_WAV_SPEECH_PATH)\n",
    "print(np.shape(waveform))\n",
    "plt.clf()\n",
    "print_stats(waveform, sample_rate=sample_rate)\n",
    "plot_waveform(waveform, sample_rate)\n",
    "plot_specgram_from_wave(waveform, sample_rate)\n",
    "#play_audio(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_specgram(spec, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "  num_freq, num_frames = spec.shape\n",
    "  time_axis = np.arange(0, num_frames) / sample_rate\n",
    "  freq_axis = np.arange(0, num_freq) * sample_rate/2/num_freq\n",
    "  figure, axes = plt.subplots(1, 1)\n",
    "  axes.pcolormesh(time_axis, freq_axis, spec[:,:], cmap='viridis')\n",
    "  axes.set_xlim(xlim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./Dataset/array/male.npy', allow_pickle=True)\n",
    "\n",
    "data[0].shape\n",
    "\n",
    "plot_specgram(data[0], sr)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4106efd4fab92acd1a2cf26cef49f6c658e7505af8b90a945581507503ac471"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('VAE-Speech')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
