{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import IPython\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "from utils import f0, extract_formants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop=256               #hop size (window size = 4*hop)\n",
    "sr=16000             #sampling rate\n",
    "n_mels= hop\n",
    "n_stft= (513-1)*2\n",
    "min_level_db=-100     #reference values to normalize data\n",
    "ref_level_db=20\n",
    "\n",
    "\n",
    "shape=128           #length of time axis of split specrograms         \n",
    "spec_split=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchaudio.transforms import Spectrogram\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "specobj = Spectrogram(n_fft=4*hop, win_length=4*hop, hop_length=hop, pad=0, power=2, normalized=False)\n",
    "specfunc = specobj.forward\n",
    "\n",
    "def specfunc(waveform):\n",
    "  specgram = specfunc(waveform)\n",
    "  return specgram\n",
    "\n",
    "def normalize(S):\n",
    "  return np.clip((((S - min_level_db) / -min_level_db)*2.)-1., -1, 1)\n",
    "\n",
    "def prep(wv, hop=192):\n",
    "  S = np.array(torch.squeeze(specfunc(torch.Tensor(wv).view(1,-1))).detach().cpu())\n",
    "  S = librosa.power_to_db(S)-ref_level_db\n",
    "  return normalize(S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate spectrograms from waveform array\n",
    "def tospec(data):\n",
    "  spectro = []\n",
    "  for awv in data:\n",
    "    spec = prep(awv)\n",
    "    spectro.append(spec)\n",
    "  return np.array(spectro, dtype=np.float32)\n",
    "    \n",
    "\n",
    "## Waveform array from path of folder containing wav files\n",
    "# def audio_array(path, gender):\n",
    "#   ls = glob(f'{path}/*.wav')\n",
    "#   adata = []\n",
    "#   ascr = []\n",
    "#   aftr = []\n",
    "\n",
    "#   for i in range(len(ls)):\n",
    "#     x, sr = tf.audio.decode_wav(tf.io.read_file(ls[i]), 1)\n",
    "#     x = np.array(x, dtype=np.float32)\n",
    "\n",
    "#     time = 1\n",
    "#     length = len(x)/sr\n",
    "\n",
    "#     while time < length:\n",
    "#       sample = x[(time-1)*sr:time*sr]\n",
    "#       src = f0(sample, gender)\n",
    "#       ftr = extract_formants(sample, gender)\n",
    "#       adata.append(sample)\n",
    "#       ascr.append(src)\n",
    "#       ftr.append(ftr)\n",
    "#       time += 1\n",
    "\n",
    "#   return np.array(adata), np.array(ascr), np.array(aftr)\n",
    "\n",
    "def single_split(audio, from_min, to_max):\n",
    "    t1 = from_min * 1000\n",
    "    t2 = to_max * 1000\n",
    "    split_audio = audio[t1:t2]\n",
    "    return split_audio\n",
    "\n",
    "\n",
    "def audio_array(path, gender, split_interval=1):\n",
    "    adata = []\n",
    "    ascr = []\n",
    "    aftr = []\n",
    "\n",
    "    for f in listdir(path):\n",
    "        if isfile(join(path, f)) and re.match(r'(.)*\\.wav\\b', f):\n",
    "            audio = AudioSegment.from_wav(f)\n",
    "            total_sec = math.floor(audio.duration_seconds)\n",
    "            \n",
    "            for i in range(0, total_sec, split_interval):\n",
    "                oneSecAudio = single_split(audio, i, i+split_interval)\n",
    "                sound = parselmouth.Sound(oneSecAudio) # Transform the file into a parselmouth object sound\n",
    "                adata.append(oneSecAudio) \n",
    "                ascr.append(f0(sound, gender))\n",
    "                aftr.append(extract_formants(sound, gender))\n",
    "\n",
    "    return np.array(adata), np.array(ascr), np.array(aftr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "PraatError",
     "evalue": "To analyse this Sound, “minimum pitch” must not be less than 132300 Hz.\nSound \"untitled\": pitch analysis not performed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPraatError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb#ch0000005?line=3'>4</a>\u001b[0m audio_directory \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./Dataset/wav/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m g\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb#ch0000005?line=4'>5</a>\u001b[0m array_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./Dataset/array/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m g\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb#ch0000005?line=6'>7</a>\u001b[0m awv, source, formant \u001b[39m=\u001b[39m audio_array(audio_directory, g)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb#ch0000005?line=7'>8</a>\u001b[0m aspec \u001b[39m=\u001b[39m tospec(awv)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb#ch0000005?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(aspec\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;32m/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb Cell 5'\u001b[0m in \u001b[0;36maudio_array\u001b[0;34m(path, gender)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb#ch0000004?line=21'>22</a>\u001b[0m length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x)\u001b[39m/\u001b[39msr\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb#ch0000004?line=23'>24</a>\u001b[0m sample \u001b[39m=\u001b[39m x\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb#ch0000004?line=24'>25</a>\u001b[0m src \u001b[39m=\u001b[39m f0(sample, gender)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb#ch0000004?line=25'>26</a>\u001b[0m ftr \u001b[39m=\u001b[39m extract_formants(sample, gender)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/generate_dataset.ipynb#ch0000004?line=26'>27</a>\u001b[0m adata\u001b[39m.\u001b[39mappend(sample)\n",
      "File \u001b[0;32m~/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/utils.py:110\u001b[0m, in \u001b[0;36mf0\u001b[0;34m(x, gender)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/utils.py?line=106'>107</a>\u001b[0m     f0max \u001b[39m=\u001b[39m \u001b[39m350\u001b[39m\n\u001b[1;32m    <a href='file:///home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/utils.py?line=108'>109</a>\u001b[0m sound \u001b[39m=\u001b[39m parselmouth\u001b[39m.\u001b[39mSound(x) \u001b[39m# read the sound\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/utils.py?line=109'>110</a>\u001b[0m pitch \u001b[39m=\u001b[39m praat\u001b[39m.\u001b[39;49mcall(sound, \u001b[39m\"\u001b[39;49m\u001b[39mTo Pitch\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0.0\u001b[39;49m, f0min, f0max) \u001b[39m# create a praat pitch object\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/utils.py?line=110'>111</a>\u001b[0m pitch_values \u001b[39m=\u001b[39m pitch\u001b[39m.\u001b[39mselected_array[\u001b[39m'\u001b[39m\u001b[39mfrequency\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    <a href='file:///home/kobemoerman/Documents/kth/year1/speech/project/VAE-Speech-Decomposition/utils.py?line=112'>113</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pitch_values\n",
      "\u001b[0;31mPraatError\u001b[0m: To analyse this Sound, “minimum pitch” must not be less than 132300 Hz.\nSound \"untitled\": pitch analysis not performed."
     ]
    }
   ],
   "source": [
    "gender = ['male', 'female']\n",
    "\n",
    "for g in gender:\n",
    "  audio_directory = './Dataset/wav/' + g\n",
    "  array_file = './Dataset/array/' + g\n",
    "\n",
    "  awv, source, formant = audio_array(audio_directory, g)\n",
    "  aspec = tospec(awv)\n",
    "\n",
    "  print(aspec.shape)\n",
    "\n",
    "  np.save(array_file + '_spec', aspec)\n",
    "  np.save(array_file + '_f0', source)\n",
    "  np.save(array_file + '_frt', formant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_stats, plot_waveform, plot_specgram_from_wave, plot_specgram\n",
    "import torchaudio\n",
    "\n",
    "SAMPLE_WAV_SPEECH_PATH = \"./Dataset/wav/female/arctic_a0001.wav\"\n",
    "\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(SAMPLE_WAV_SPEECH_PATH)\n",
    "print(np.shape(waveform))\n",
    "plt.clf()\n",
    "print_stats(waveform, sample_rate=sample_rate)\n",
    "plot_waveform(waveform, sample_rate)\n",
    "plot_specgram_from_wave(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "def to_audio(x):\n",
    "  audio_signal = librosa.core.spectrum.griffinlim(abs_spectrogram)\n",
    "  return audio_signal\n",
    "\n",
    "\"\"\" Spectrogram transformation without normalisation \"\"\"\n",
    "sig, fs = librosa.core.load(\"./Dataset/wav/male/arctic_a0001.wav\", sr=16000)\n",
    "abs_spectrogram = np.abs(librosa.core.spectrum.stft(sig))\n",
    "plot_specgram(abs_spectrogram, 16000)\n",
    "audio_signal = librosa.core.spectrum.griffinlim(abs_spectrogram)\n",
    "sf.write('test_normal.wav', data=audio_signal, samplerate=16000)\n",
    "\n",
    "audio_signal = awv[0]\n",
    "sf.write('test_sec_cut.wav', data=audio_signal, samplerate=16000)\n",
    "\n",
    "\n",
    "\n",
    "plot_specgram(aspec[0], sr)\n",
    "audio_signal = librosa.core.spectrum.griffinlim(aspec[0])\n",
    "sf.write('test_normalised.wav', data=audio_signal, samplerate=16000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4106efd4fab92acd1a2cf26cef49f6c658e7505af8b90a945581507503ac471"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('VAE-Speech')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
