{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anne/Desktop/VAE_Speech_Decomposition/speechTech/lib/python3.7/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import IPython\n",
    "import time\n",
    "import librosa\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import torchaudio\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "\n",
    "import parselmouth \n",
    "from parselmouth import praat\n",
    "\n",
    "from utils import f0, extract_formants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop=256               #hop size (window size = 4*hop)\n",
    "sr=16000             #sampling rate\n",
    "min_level_db=-100     #reference values to normalize data\n",
    "ref_level_db=20\n",
    "\n",
    "\n",
    "shape=128           #length of time axis of split specrograms         \n",
    "spec_split=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchaudio.transforms import MelScale, Spectrogram\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "specobj = Spectrogram(n_fft=4*hop, win_length=4*hop, hop_length=hop, pad=0, power=2, normalized=False)\n",
    "specfunc = specobj.forward\n",
    "\n",
    "def melspecfunc(waveform):\n",
    "  specgram = specfunc(waveform)\n",
    "  #mel_specgram = melfunc(specgram)\n",
    "  #return mel_specgram\n",
    "  return specgram\n",
    "\n",
    "def normalize(S):\n",
    "  return np.clip((((S - min_level_db) / -min_level_db)*2.)-1., -1, 1)\n",
    "\n",
    "def prep(wv, hop=192):\n",
    "  S = np.array(torch.squeeze(melspecfunc(torch.Tensor(wv).view(1,-1))).detach().cpu())\n",
    "  S = librosa.power_to_db(S)-ref_level_db\n",
    "  return normalize(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split audios into chunks of appropriate duration \n",
    "class SplitWavAudio():\n",
    "    '''\n",
    "    Split sounds into smaller chunks\n",
    "    '''\n",
    "    def __init__(self, folder, filename):\n",
    "        self.folder = folder\n",
    "        self.new_folder = folder + '_cut'\n",
    "        self.filename = filename\n",
    "        self.filepath = folder + '/' + filename\n",
    "        \n",
    "        self.audio = AudioSegment.from_wav(self.filepath)\n",
    "    \n",
    "    def get_duration(self):\n",
    "        return self.audio.duration_seconds\n",
    "    \n",
    "    def single_split(self, from_min, to_max, split_filename):\n",
    "        t1 = from_min * 1000\n",
    "        t2 = to_max * 1000\n",
    "        split_audio = self.audio[t1:t2]\n",
    "        split_audio.export(self.new_folder + '/' + split_filename, format=\"wav\")\n",
    "        \n",
    "    def multiple_split(self, split_interval):\n",
    "        total_sec = math.floor(self.get_duration())\n",
    "        for i in range(0, total_sec, split_interval):\n",
    "            split_fn = str(i) + '_' + self.filename\n",
    "            self.single_split(i, i+split_interval, split_fn)\n",
    "            print(str(i) + ' Done')\n",
    "            if i == total_sec - split_interval:\n",
    "                print('All splited successfully')\n",
    "\n",
    "\n",
    "def folder_split(folder, split_interval=1):\n",
    "    '''\n",
    "    Apply SplitWavAudio methods to a directory's files\n",
    "    '''\n",
    "    for f in listdir(folder):\n",
    "        if isfile(join(folder, f)) and re.match(r'(.)*\\.wav\\b', f):\n",
    "            split_wav = SplitWavAudio(folder, f)\n",
    "            split_wav.multiple_split(split_interval)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = ['male', 'female']\n",
    "\n",
    "for g in gender:\n",
    "    repository = 'Dataset/wav/' + g\n",
    "    chunks_repository = repository + '_cut'\n",
    "    if not os.path.exists(chunks_repository):\n",
    "        os.makedirs(chunks_repository)\n",
    "        folder_split(repository , split_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_array(path, gender, array_file):\n",
    "    adata = []\n",
    "    ascr = []\n",
    "    aftr = []\n",
    "\n",
    "    for f in listdir(path):\n",
    "        audio_path = join(path, f)\n",
    "        i = 0\n",
    "        nb_points = 100\n",
    "        less_points_file = audio_path\n",
    "        if isfile(audio_path) and re.match(r'(.)*\\.wav\\b', f):\n",
    "            i += 1\n",
    "            awv, sr = tf.audio.decode_wav(tf.io.read_file(audio_path), 1)\n",
    "            awv = np.array(awv, dtype=np.float32)\n",
    "            spec = prep(awv)\n",
    "            adata.append(spec)\n",
    "\n",
    "            sound = parselmouth.Sound(audio_path) # Transform  the file into a parselmouth object sound\n",
    "            src = f0(sound, gender)\n",
    "            frt, nb = extract_formants(sound, gender)\n",
    "            frt = np.nan_to_num(frt, copy=False, nan=0.0)\n",
    "            ascr.append(src)\n",
    "            aftr.append(frt)   \n",
    "\n",
    "            if nb <  nb_points:\n",
    "                nb_points = nb\n",
    "                less_points_file = audio_path   \n",
    "    \n",
    "    adata, ascr, aftr = np.array(adata), np.array(ascr), np.array(aftr)\n",
    "\n",
    "    np.save(array_file + '_spec', adata)\n",
    "    np.save(array_file + '_f0', ascr)\n",
    "    np.save(array_file + '_frt', aftr)\n",
    "\n",
    "    print(adata.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = ['male', 'female']\n",
    "\n",
    "for g in gender:\n",
    "    audio_directory = './Dataset/wav/' + g + '_cut'\n",
    "    array_file = './Dataset/array/' + g\n",
    "    spec_array(audio_directory, gender, array_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10a5e452c7f8a0f4069a1f6307d1514c39786ac2ed1a9dfc3e9cb58580672aef"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('speechTech': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
