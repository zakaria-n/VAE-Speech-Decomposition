{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import IPython\n",
    "import time\n",
    "import librosa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 #@param {type:\"raw\"}\n",
    "num_epochs_to_train =  10#@param {type:\"integer\"}\n",
    "batch_size =  32#@param {type:\"integer\"}\n",
    "vector_dimension = 64 #@param {type:\"integer\"}\n",
    "\n",
    "hop=256               #hop size (window size = 4*hop)\n",
    "sr=44100              #sampling rate\n",
    "min_level_db=-100     #reference values to normalize data\n",
    "ref_level_db=20\n",
    "\n",
    "\n",
    "shape=128           #length of time axis of split specrograms         \n",
    "spec_split=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchaudio.transforms import MelScale, Spectrogram\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "specobj = Spectrogram(n_fft=4*hop, win_length=4*hop, hop_length=hop, pad=0, power=2, normalized=False)\n",
    "specfunc = specobj.forward\n",
    "melobj = MelScale(n_mels=hop, sample_rate=sr, f_min=0.)\n",
    "melfunc = melobj.forward\n",
    "\n",
    "def melspecfunc(waveform):\n",
    "  specgram = specfunc(waveform)\n",
    "  #mel_specgram = melfunc(specgram)\n",
    "  #return mel_specgram\n",
    "  return specgram\n",
    "\n",
    "def spectral_convergence(input, target):\n",
    "    return 20 * ((input - target).norm().log10() - target.norm().log10())\n",
    "\n",
    "def normalize(S):\n",
    "  return np.clip((((S - min_level_db) / -min_level_db)*2.)-1., -1, 1)\n",
    "\n",
    "def denormalize(S):\n",
    "  return (((np.clip(S, -1, 1)+1.)/2.) * -min_level_db) + min_level_db\n",
    "\n",
    "def prep(wv, hop=192):\n",
    "  S = np.array(torch.squeeze(melspecfunc(torch.Tensor(wv).view(1,-1))).detach().cpu())\n",
    "  S = librosa.power_to_db(S)-ref_level_db\n",
    "  return normalize(S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate spectrograms from waveform array\n",
    "def tospec(data):\n",
    "  specs=np.empty(data.shape[0], dtype=object)\n",
    "  for i in range(data.shape[0]):\n",
    "    x = data[i]\n",
    "    S = prep(x)\n",
    "    S = np.array(S, dtype=np.float32)\n",
    "    specs[i]=np.expand_dims(S, -1)\n",
    "  print(specs.shape)\n",
    "  return specs\n",
    "\n",
    "#Generate multiple spectrograms with a determined length from single wav file\n",
    "def tospeclong(path, length=4*44100):\n",
    "  x, sr = librosa.load(path,sr=44100)\n",
    "  x,_ = librosa.effects.trim(x)\n",
    "  loudls = librosa.effects.split(x, top_db=50)\n",
    "  xls = np.array([])\n",
    "  for interv in loudls:\n",
    "    xls = np.concatenate((xls,x[interv[0]:interv[1]]))\n",
    "  x = xls\n",
    "  num = x.shape[0]//length\n",
    "  specs=np.empty(num, dtype=object)\n",
    "  for i in range(num-1):\n",
    "    a = x[i*length:(i+1)*length]\n",
    "    S = prep(a)\n",
    "    S = np.array(S, dtype=np.float32)\n",
    "    try:\n",
    "      sh = S.shape\n",
    "      specs[i]=S\n",
    "    except AttributeError:\n",
    "      print('spectrogram failed')\n",
    "  print(specs.shape)\n",
    "  return specs\n",
    "\n",
    "#Waveform array from path of folder containing wav files\n",
    "def audio_array(path):\n",
    "  ls = glob(f'{path}/*.wav')\n",
    "  adata = []\n",
    "  for i in range(len(ls)):\n",
    "    x, sr = tf.audio.decode_wav(tf.io.read_file(ls[i]), 1)\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    adata.append(x)\n",
    "  return np.array(adata)\n",
    "\n",
    "#Waveform array from path of folder containing wav files\n",
    "def single_audio_array(path):\n",
    "  ls = glob(path)\n",
    "  adata = []\n",
    "  for i in range(len(ls)):\n",
    "    x, sr = tf.audio.decode_wav(tf.io.read_file(ls[i]), 1)\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    adata.append(x)\n",
    "  return np.array(adata)\n",
    "\n",
    "\n",
    "#Concatenate spectrograms in array along the time axis\n",
    "def testass(a):\n",
    "  but=False\n",
    "  con = np.array([])\n",
    "  nim = a.shape[0]\n",
    "  for i in range(nim):\n",
    "    im = a[i]\n",
    "    im = np.squeeze(im)\n",
    "    if not but:\n",
    "      con=im\n",
    "      but=True\n",
    "    else:\n",
    "      con = np.concatenate((con,im), axis=1)\n",
    "  return np.squeeze(con)\n",
    "\n",
    "#Split spectrograms in chunks with equal size\n",
    "def splitcut(data):\n",
    "  ls = []\n",
    "  mini = 0\n",
    "  minifinal = spec_split*shape   #max spectrogram length\n",
    "  for i in range(data.shape[0]-1):\n",
    "    if data[i].shape[1]<=data[i+1].shape[1]:\n",
    "      mini = data[i].shape[1]\n",
    "    else:\n",
    "      mini = data[i+1].shape[1]\n",
    "    if mini>=3*shape and mini<minifinal:\n",
    "      minifinal = mini\n",
    "  for i in range(data.shape[0]):\n",
    "    x = data[i]\n",
    "    if x.shape[1]>=3*shape:\n",
    "      for n in range(x.shape[1]//minifinal):\n",
    "        ls.append(x[:,n*minifinal:n*minifinal+minifinal,:])\n",
    "      ls.append(x[:,-minifinal:,:])\n",
    "  return np.array(ls)\n",
    "\n",
    "# Generates timestamp string of \"day_month_year_hourMin\" \n",
    "def get_time_stamp():\n",
    "  secondsSinceEpoch = time.time()\n",
    "  timeObj = time.localtime(secondsSinceEpoch)\n",
    "  x = ('%d_%d_%d_%d%d' % (timeObj.tm_mday, timeObj.tm_mon, timeObj.tm_year, timeObj.tm_hour, timeObj.tm_min))\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\VAE-Speech\\lib\\site-packages\\ipykernel_launcher.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1132,)\n",
      "(1132,)\n",
      "(1132,)\n"
     ]
    }
   ],
   "source": [
    "#Generating Mel-Spectrogram dataset (mel is commented, only specto here)\n",
    "#adata: source spectrograms\n",
    "\n",
    "audio_directory = \"./Dataset/wav/female\" #@param {type:\"string\"}\n",
    "array_file = './Dataset/array/female'\n",
    "\n",
    "#AUDIO TO CONVERT\n",
    "awv = audio_array(audio_directory) #get waveform array from folder containing wav files\n",
    "print(np.shape(awv))        \n",
    "aspec = tospec(awv)                      #get spectrogram array\n",
    "print(np.shape(aspec))\n",
    "#adata = splitcut(aspec)                    #split spectrogams to fixed (not used here)\n",
    "#print(np.shape(adata))\n",
    "np.save(array_file,aspec)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4106efd4fab92acd1a2cf26cef49f6c658e7505af8b90a945581507503ac471"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('VAE-Speech')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
