{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import IPython\n",
    "import time\n",
    "import librosa\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import torchaudio\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "\n",
    "import parselmouth \n",
    "from parselmouth import praat\n",
    "\n",
    "from utils import f0, extract_formants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop=256               #hop size (window size = 4*hop)\n",
    "sr=16000             #sampling rate\n",
    "min_level_db=-100     #reference values to normalize data\n",
    "ref_level_db=20\n",
    "\n",
    "\n",
    "shape=128           #length of time axis of split specrograms         \n",
    "spec_split=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchaudio.transforms import MelScale, Spectrogram\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "specobj = Spectrogram(n_fft=4*hop, win_length=4*hop, hop_length=hop, pad=0, power=2, normalized=False)\n",
    "specfunc = specobj.forward\n",
    "\n",
    "def melspecfunc(waveform):\n",
    "  specgram = specfunc(waveform)\n",
    "  #mel_specgram = melfunc(specgram)\n",
    "  #return mel_specgram\n",
    "  return specgram\n",
    "\n",
    "def normalize(S):\n",
    "  return np.clip((((S - min_level_db) / -min_level_db)*2.)-1., -1, 1)\n",
    "\n",
    "def prep(wv, hop=192):\n",
    "  S = np.array(torch.squeeze(melspecfunc(torch.Tensor(wv).view(1,-1))).detach().cpu())\n",
    "  S = librosa.power_to_db(S)-ref_level_db\n",
    "  return normalize(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split audios into chunks of appropriate duration \n",
    "class SplitWavAudio():\n",
    "    '''\n",
    "    Split sounds into smaller chunks\n",
    "    '''\n",
    "    def __init__(self, folder, filename):\n",
    "        self.folder = folder\n",
    "        self.new_folder = folder + '_cut'\n",
    "        self.filename = filename\n",
    "        self.filepath = folder + '/' + filename\n",
    "        \n",
    "        self.audio = AudioSegment.from_wav(self.filepath)\n",
    "    \n",
    "    def get_duration(self):\n",
    "        return self.audio.duration_seconds\n",
    "    \n",
    "    def single_split(self, from_min, to_max, split_filename):\n",
    "        t1 = from_min * 1000\n",
    "        t2 = to_max * 1000\n",
    "        split_audio = self.audio[t1:t2]\n",
    "        split_audio.export(self.new_folder + '/' + split_filename, format=\"wav\")\n",
    "        \n",
    "    def multiple_split(self, split_interval):\n",
    "        total_sec = math.floor(self.get_duration())\n",
    "        for i in range(0, total_sec, split_interval):\n",
    "            split_fn = str(i) + '_' + self.filename\n",
    "            self.single_split(i, i+split_interval, split_fn)\n",
    "            print(str(i) + ' Done')\n",
    "            if i == total_sec - split_interval:\n",
    "                print('All splited successfully')\n",
    "\n",
    "\n",
    "def folder_split(folder, split_interval=1):\n",
    "    '''\n",
    "    Apply SplitWavAudio methods to a directory's files\n",
    "    '''\n",
    "    for f in listdir(folder):\n",
    "        if isfile(join(folder, f)) and re.match(r'(.)*\\.wav\\b', f):\n",
    "            split_wav = SplitWavAudio(folder, f)\n",
    "            split_wav.multiple_split(split_interval)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = ['male', 'female']\n",
    "\n",
    "for g in gender:\n",
    "    repository = 'Dataset/wav/' + g\n",
    "    chunks_repository = repository + '_cut'\n",
    "    if not os.path.exists(chunks_repository):\n",
    "        os.makedirs(chunks_repository)\n",
    "        folder_split(repository , split_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_array(path, gender, array_file):\n",
    "    adata = []\n",
    "    ascr = []\n",
    "    aftr = []\n",
    "\n",
    "    for f in listdir(path):\n",
    "        audio_path = join(path, f)\n",
    "        i = 0\n",
    "        nb_points = 400\n",
    "        if isfile(audio_path) and re.match(r'(.)*\\.wav\\b', f):\n",
    "            i += 1\n",
    "            awv, sr = tf.audio.decode_wav(tf.io.read_file(audio_path), 1)\n",
    "            awv = np.array(awv, dtype=np.float32)\n",
    "            spec = prep(awv)\n",
    "            adata.append(spec)\n",
    "\n",
    "            sound = parselmouth.Sound(audio_path) # Transform  the file into a parselmouth object sound\n",
    "            src = f0(sound, gender)\n",
    "            frt, nb = extract_formants(sound, gender)\n",
    "            # frt = np.nan_to_num(frt, copy=False, nan=0.0)\n",
    "            ascr.append(src)\n",
    "            aftr.append(frt)   \n",
    "\n",
    "            if nb >  nb_points:\n",
    "                nb_points = nb \n",
    "\n",
    "    aftr = pad_formants(aftr, nb_points)\n",
    "    \n",
    "    adata, ascr, aftr = np.array(adata), np.array(ascr), np.array(aftr)\n",
    "\n",
    "    np.save(array_file + '_spec', adata)\n",
    "    np.save(array_file + '_f0', ascr)\n",
    "    np.save(array_file + '_frt', aftr)\n",
    "\n",
    "    print(adata.shape)\n",
    "\n",
    "\n",
    "def reduce_formants(formant, nb_points):\n",
    "    reduce_frt = []\n",
    "    for arr in formant:\n",
    "        reduced = arr[:nb_points]\n",
    "        reduced = np.nan_to_num(reduced, copy=False, nan=0.0)\n",
    "        reduce_frt.append(reduced)\n",
    "    return np.array(reduce_frt)\n",
    "\n",
    "def pad_formants(formant, nb_points):\n",
    "    padded_frt = []\n",
    "    for arr in formant:\n",
    "        padded = np.pad(arr, (0, 438-len(arr)), 'constant')\n",
    "        padded = np.nan_to_num(padded, copy=False, nan=0.0)\n",
    "        padded_frt.append(padded)\n",
    "    return np.array(padded_frt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 9 5 4 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "  \n",
    "  \n",
    "arr = [1, 3, 9, 5, 4]\n",
    "  \n",
    "# padding array using 'maximum' mode\n",
    "pad_arr = np.pad(arr, (0,5), 'constant')\n",
    "  \n",
    "print(pad_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6527/3503110013.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  adata, ascr, aftr = np.array(adata), np.array(ascr), np.array(aftr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2489, 513, 63)\n"
     ]
    }
   ],
   "source": [
    "gender = ['male', 'female']\n",
    "\n",
    "g = gender[0]\n",
    "audio_directory = './Dataset/wav/' + g + '_cut'\n",
    "array_file = './Dataset/array/' + g\n",
    "spec_array(audio_directory, gender, array_file)\n",
    "\n",
    "# for g in gender:\n",
    "#     audio_directory = './Dataset/wav/' + g + '_cut'\n",
    "#     array_file = './Dataset/array/' + g\n",
    "#     spec_array(audio_directory, gender, array_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = './Dataset/array/'\n",
    "\n",
    "f0 = np.load(audio_dir + 'male_f0.npy', allow_pickle=True)\n",
    "frt = np.load(audio_dir + 'male_frt.npy', allow_pickle=True)\n",
    "spec = np.load(audio_dir + 'male_spec.npy', allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2489, 513, 63)\n",
      "(2489, 197)\n",
      "(2489,)\n",
      "(91,)\n"
     ]
    }
   ],
   "source": [
    "print(spec.shape)\n",
    "print(f0.shape)\n",
    "\n",
    "default = f0[0].shape\n",
    "\n",
    "# for e in f0:\n",
    "#   if e.shape != default:\n",
    "#     print(e.shape)\n",
    "\n",
    "print(frt.shape)\n",
    "print(frt[0].shape)\n",
    "# for e in frt:\n",
    "#   print(e.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10a5e452c7f8a0f4069a1f6307d1514c39786ac2ed1a9dfc3e9cb58580672aef"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('speechTech': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
