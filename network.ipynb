{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "This version of TensorFlow Probability requires TensorFlow version >= 2.8; Detected an installation of version 2.6.2. Please upgrade TensorFlow to proceed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fbe079db6f7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMeanSquaredError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\VAE-Speech\\lib\\site-packages\\tensorflow_probability\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# `python/__init__.py` as necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msubstrates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\VAE-Speech\\lib\\site-packages\\tensorflow_probability\\substrates\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;34m\"\"\"TensorFlow Probability alternative substrates.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mall_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlazy_loader\u001b[0m  \u001b[1;31m# pylint: disable=g-direct-tensorflow-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\VAE-Speech\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    136\u001b[0m   \u001b[1;31m# Non-lazy load of packages that register with tensorflow or keras.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mpkg_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_maybe_nonlazy_load\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpkg_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Forces loading the package from its lazy loader.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\VAE-Speech\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py\u001b[0m in \u001b[0;36m__dir__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\VAE-Speech\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_on_first_access\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_on_first_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_on_first_access\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\VAE-Speech\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py\u001b[0m in \u001b[0;36m_validate_tf_environment\u001b[1;34m(package)\u001b[0m\n\u001b[0;32m     62\u001b[0m         'Please upgrade TensorFlow to proceed.'.format(\n\u001b[0;32m     63\u001b[0m             \u001b[0mrequired\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequired_tensorflow_version\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             present=tf.__version__))\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m   if (package == 'mcmc' and\n",
      "\u001b[1;31mImportError\u001b[0m: This version of TensorFlow Probability requires TensorFlow version >= 2.8; Detected an installation of version 2.6.2. Please upgrade TensorFlow to proceed."
     ]
    }
   ],
   "source": [
    "# Import Tensorflow and Numpy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda, Cropping2D, ZeroPadding2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2489, 256, 63)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = './Dataset/array'\n",
    "\n",
    "data = np.load(dir + '/male.npy', allow_pickle=True)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (512, 64, 1) \n",
    "conv_filters=(512, 256, 128, 64, 32)\n",
    "conv_kernels=(3, 3, 3, 3, 3)\n",
    "conv_strides=(2, 2, 2, 2, (2,1))\n",
    "vector_dimension = 64\n",
    "latent_space_dim = vector_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parallel_CVAE(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Parallel_CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.alpha = # regularization factors\n",
    "        self.lr = 0.0001\n",
    "\n",
    "        self.e1 = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=conv_filters, kernel_size=conv_kernels, strides=conv_strides, activation='relu'),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=conv_filters, kernel_size=conv_kernels, strides=conv_strides, activation='relu'),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                # No activation\n",
    "                tf.keras.layers.Dense(latent_space_dim + latent_space_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.e2 = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=conv_filters, kernel_size=conv_kernels, strides=conv_strides, activation='relu'),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=conv_filters, kernel_size=conv_kernels, strides=conv_filters, activation='relu'),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                # No activation\n",
    "                tf.keras.layers.Dense(latent_space_dim + latent_space_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        self.d1 = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(latent_space_dim,)),\n",
    "                tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                #tf.keras.layers.Reshape(target_shape=(7, 7, 32)),  # TODO\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=conv_filters, kernel_size=conv_kernels, strides=conv_filters, padding='same',\n",
    "                    activation='relu'),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=conv_filters, kernel_size=conv_kernels, strides=conv_strides, padding='same',\n",
    "                    activation='relu'),\n",
    "                # No activation\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=conv_filters, kernel_size=conv_kernels, strides=conv_strides, padding='same'),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.d2 = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(latent_space_dim,)),\n",
    "                tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                #tf.keras.layers.Reshape(target_shape=(7, 7, 32)),  # TODO\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=conv_filters, kernel_size=conv_kernels, strides=conv_strides, padding='same',\n",
    "                    activation='relu'),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=conv_filters, kernel_size=conv_kernels, strides=conv_strides, padding='same',\n",
    "                    activation='relu'),\n",
    "                # No activation\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=conv_filters, kernel_size=conv_kernels, strides=conv_strides, padding='same'),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self, x_train, f0_train, formant_train, y_train):\n",
    "        # init input variable\n",
    "        x = tf.Variable(x_train, name=\"X\", dtype=\"float32\")\n",
    "\n",
    "        # calculate f0 loss\n",
    "        f0_target = tf.Variable(f0_train, name=\"F0\", dtype=\"float32\")\n",
    "        f0_predicted = #f0(self.autoencode_z1(x)) %TODO\n",
    "        f0_Loss = tf.nn.l2_loss(f0_predicted-f0_target)\n",
    "\n",
    "        # Calculate formant loss\n",
    "        formant_target = tf.Variable(formant_train, name=\"Formant\", dtype=\"float32\") \n",
    "        formant_predicted = #formant(self.autoencode_z2(x)) %TODO\n",
    "        formant_Loss = tf.nn.l2_loss(formant_predicted-formant_target)\n",
    "        \n",
    "        # calculate reconstruction loss with KL\n",
    "        y_target = tf.Variable(y_train, name=\"Y\", dtype=\"float32\")\n",
    "        y_predicted = self.autoencode(x)\n",
    "        reconstruction_Loss = tf.nn.l2_loss(y_target - y_predicted)\n",
    "\n",
    "        # calculate joint_loss\n",
    "        Joint_Loss = f0_Loss + formant_Loss + reconstruction_Loss\n",
    "\n",
    "        # optimisers\n",
    "        Optimiser = tf.train.AdamOptimizer(learning_rate = self.lr).minimize(Joint_Loss)\n",
    "        # Y1_op = tf.train.AdamOptimizer().minimize(f0_Loss)\n",
    "        # Y2_op = tf.train.AdamOptimizer(learning_rate = self.lr).minimize(f0_Loss)\n",
    "        \n",
    "        # run training\n",
    "        with tf.Session() as session:\n",
    "            session.run(tf.initialize_all_variables())\n",
    "            _, Joint_Loss = session.run([Optimiser, Joint_Loss])\n",
    "            print(Joint_Loss)\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, mean, logvar ):\n",
    "        eps = tfp.distributions.Normal(shape=mean.shape).sample()     \n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def encode_distrib_z1(self, x):\n",
    "        mean, logvar = tf.split(self.e1(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def encode_z1(self, x):\n",
    "        return self.sample(self.encode_distrib_z1(x))\n",
    "\n",
    "    def encode_distrib_z2(self, x):\n",
    "        mean, logvar = tf.split(self.e2(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def encode_z2(self, x):\n",
    "        return self.sample(self.encode_distrib_z2(x))\n",
    "\n",
    "    def decode_z1(self, z1, apply_sigmoid=False):\n",
    "        logits = self.d1(z1)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "    def decode_z2(self, z2, apply_sigmoid=False):\n",
    "        logits = self.d2(z2)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "    def decode(self, z1, z2):\n",
    "        logits1 = self.d1(z1)\n",
    "        logits2 = self.d2(z2)\n",
    "        return logits1 + logits2\n",
    "    \n",
    "    def autoencode_z1(self, x):\n",
    "        z1 = self.encode_z1(x)\n",
    "        y1 = self.decode_z1(z1)\n",
    "        return y1\n",
    "    \n",
    "    def autoencode_z2(self, x):\n",
    "        z1 = self.encode_z2(x)\n",
    "        y1 = self.decode_z2(z1)\n",
    "        return y1\n",
    "    \n",
    "    def autoencode(self, x):\n",
    "        y1 = self.autoencode_z1(x)\n",
    "        y2 = self.autoencode_z2(x)\n",
    "        return y1 + y2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4106efd4fab92acd1a2cf26cef49f6c658e7505af8b90a945581507503ac471"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('VAE-Speech')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
